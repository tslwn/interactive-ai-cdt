\section{Probabilistic models}
\label{sec:9:probabilistic-models}

Recall that:
\begin{itemize}
  \item $P(Y \mid X)$ is the posterior probability distribution of $Y$ given $X$;
  \item $P(Y, X)$ is the joint probability distribution of $Y$ and $X$;
  \item $P(X \mid Y)$ is the likelihood function; and
  \item $P(X)$ is the prior distribution of $X$.
\end{itemize}

\paragraph{Discriminative and generative models}

\begin{itemize}
  \item A \textit{discriminative} model describes the posterior distribution of
        the target given the input.
        It does not describe the prior distribution of the input.
  \item A \textit{generative} model describes a joint distribution of the target
        and input.
        If the prior distribution of the target can be estimated, then it can be
        described by a likelihood function.
        It can be used to generate data by sampling from the joint distribution.
\end{itemize}

Generative models can do more than discriminative models.
However, joint distributions are harder to learn than conditional distributions
like the posterior distribution because they are described by more probability
values (parameters).
This may be handled by simplifying assumptions like independence but they are
not always appropriate.

\paragraph{Uncertainty}

A probabilistic view treats learning as a procedure that reduces uncertainty.

\subsection{Normal distributions}

\begin{dfn}[Univariate normal distribution]
  \label{dfn:9:univariate-normal-distribution}
  Let $x \in \mathbb{R}$ be a random variable of mean $\mu$ and standard
  deviation $\sigma$.
  The normal distribution is:
  \begin{align}
    P(x \mid \mu, \sigma)
     & = \frac{1}{\sqrt{2 \pi} \sigma} \exp \left( - \frac{(x - \mu)^2}{2 \sigma^2} \right)
    \\
     & = \frac{1}{E} \exp \left( - \frac{z^2}{2} \right) ,\
    z = \frac{x - \mu}{\sigma} ,\
    E = \sqrt{2 \pi} \sigma
  \end{align}
\end{dfn}

\begin{dfn}[Standard univariate normal distribution]
  \begin{equation}
    P(x \mid \mu = 0, \sigma = 1)
    = \frac{1}{\sqrt{2 \pi}} \exp \left( - \frac{x^2}{2} \right)
  \end{equation}
\end{dfn}

\begin{dfn}[Multivariate normal distribution]
  \label{dfn:9:multivariate-normal-distribution}
  Let $\vec{x} \in \mathbb{R}^n$ be a random variable of mean $\vec{\mu}$ and
  covariance matrix $\matr{\Sigma}$.
  The normal distribution is:
  \begin{equation}
    \begin{split}
      P(\vec{x} \mid \vec{\mu}, \matr{\Sigma})
      = \frac{1}{E_n} \exp \left(
      - \frac{1}{2} (\vec{x} - \vec{\mu})^T \matr{\Sigma}^{-1} (\vec{x} - \vec{\mu})
      \right) ,
      \\
      E_n = (2 \pi)^{n/2} \sqrt{\det \matr{\Sigma}}
    \end{split}
  \end{equation}
\end{dfn}

\begin{dfn}[Standard multivariate normal distribution]
  \begin{equation}
    P(\vec{x} \mid \vec{\mu} = \vec{0}, \matr{\Sigma} = \matr{I})
    = \frac{1}{(2 \pi)^{n / 2}} \exp \left( - \frac{\norm{\vec{x}}^2}{2} \right)
  \end{equation}
\end{dfn}

\subsubsection{Gaussian mixture models}

\begin{dfn}[Mixture model]
  Let $X = \bigcup_{i = 1}^k X_i$ be a set of instances partitioned into $k$ classes.
  A mixture model is a set of probability distributions or components $\{ P(X_i)
    \mid i \in 1 .. k \}$.
\end{dfn}

\begin{dfn}[Gaussian mixture model]
  Let $X = \bigcup_{i = 1}^k X_i$ be a set of instances partitioned into $k$ classes.
  A Gaussian mixture model (GMM) is a set of Gaussian distributions $\{ P(X_i
    \mid \vec{\mu}_i, \matr{\Sigma}_i) \mid i \in 1 .. k \}$.
\end{dfn}

\begin{dfn}[Likelihood ratio of a binary classifier]
  Let $X = X_+ \cup X_-$ be a set of instances partitioned into two classes and
  $P(x \mid X_i)$ be the likelihood that an instance $x$ belongs to class $X_i$.
  The likelihood ratio is:
  \begin{equation*}
    \lambda(x) = \frac{P(x \mid X_+)}{P(x \mid X_-)}
  \end{equation*}
\end{dfn}

\begin{proposition}[Likelihood ratio of a binary univariate GMM]
  \begin{align*}
    \lambda(x)
     & = \frac{P(x \mid X_+)}{P(x \mid X_-)}
    = \frac{P(x \mid \mu_+, \sigma_+)}{P(x \mid \mu_-, \sigma_-)}
    \\
     & = \frac{\sigma_-}{\sigma_+}\exp\left(-\frac{1}{2}\left(
      \left(\frac{x - \mu_+}{\sigma_+} \right)^2 -
      \left(\frac{x - \mu_-}{\sigma_-} \right)^2
      \right)\right)
  \end{align*}
\end{proposition}

\begin{dfn}[Maximum-likelihood decision threshold of a binary classifier]
  Let $X = X_+ \cup X_-$ be a set of instances partitioned into two classes.
  The maximum-likelihood decision threshold is $x : \lambda(x) = 1$.
\end{dfn}

\begin{proposition}
  Let $\{ P(X_+ \mid \mu_+, \sigma), P(X_- \mid \mu_-, \sigma) \}$ be a binary
  univariate GMM.
  The maximum-likelihood decision threshold is $x = \frac{1}{2}(\mu_+ + \mu_-)$.
  \begin{proof}
    \begin{align*}
      \lambda(x)
       & = \exp\left(-\frac{1}{2}\left(
        \left(\frac{x - \mu_+}{\sigma} \right)^2 -
        \left(\frac{x - \mu_-}{\sigma} \right)^2
        \right)\right)
      \\[2ex]
       & = \exp\left(\frac{\mu_+ - \mu_-}{\sigma^2}\left(
        x - \frac{\mu_+ + \mu_-}{2}
        \right)\right)
    \end{align*}
    $x : \lambda(x) = 1 \implies x = \frac{1}{2}(\mu_+ + \mu_-)$.
  \end{proof}
\end{proposition}

\begin{proposition}
  Let $\{ P(X_+ \mid \mu_+, \sigma_+), P(X_- \mid \mu_-, \sigma_-) \}, \sigma_+
    \neq \sigma_-$ be a binary univariate GMM.
  There are two maximum-likelihood decision thresholds.
  \begin{proof}
    \begin{equation*}
      \lambda(x)
      = \frac{\sigma_-}{\sigma_+}\exp\left(-\frac{1}{2}\left(
        \left(\frac{x - \mu_+}{\sigma_+} \right)^2 -
        \left(\frac{x - \mu_-}{\sigma_-} \right)^2
        \right)\right)
    \end{equation*}
    \begin{align*}
      \lambda(x) = 1 \implies \frac{\sigma_+}{\sigma_-}
       & = \exp\left(-\frac{1}{2}\left(
        \left(\frac{x - \mu_+}{\sigma_+} \right)^2 -
        \left(\frac{x - \mu_-}{\sigma_-} \right)^2
        \right)\right)
      \\[2ex]
      -2 \ln \frac{\sigma_+}{\sigma_-}
       & = \left(\frac{x - \mu_+}{\sigma_+} \right)^2 -
      \left(\frac{x - \mu_-}{\sigma_-} \right)^2
    \end{align*}
    This is a quadratic equation in $x$:
    \begin{align*}
      \left(\sigma_-^2 - \sigma_+^2\right) x^2
      + 2\left(\mu_+\sigma_-^2 - \mu_-\sigma_+^2\right) x
      + \left(
      \mu_-^2\sigma_+^2 -
      \mu_+^2\sigma_-^2 -
      2\sigma_-^2\sigma_+^2\ln\frac{\sigma_+}{\sigma_-}
      \right)
       & = 0
    \end{align*}
    It can be shown that:
    \begin{equation*}
      x = \frac{1}{\sigma_+^2 - \sigma_-^2}\left(
      \mu_-\sigma_+^2 - \mu_+\sigma_-^2 \pm
      \sigma_+\sigma_-\sqrt{
          2(\sigma_+^2 - \sigma_-^2)\ln\frac{\sigma_+}{\sigma_-} +
          (\mu_+ - \mu_-)^2
        }
      \right)
    \end{equation*}
    The argument of the square-root function is non-negative; hence, there are two
    real-valued solutions for $x$.
  \end{proof}
\end{proposition}

\begin{proposition}[Likelihood ratio of a binary multivariate GMM]
  \begin{align*}
    \lambda(\vec{x})
     & = \frac{P(\vec{x} \mid X_+)}{P(\vec{x} \mid X_-)}
    \\[2ex]
     & = \sqrt{\frac{\det\Sigma_+}{\det\Sigma_-}} \exp \Bigg( -\frac{1}{2} \bigg(
      \left(\vec{x} - \vec{\mu}_+ \right)^T \Sigma_+^{-1} \left(\vec{x} - \vec{\mu}_+ \right)
    \\
     & \qquad - \left(\vec{x} - \vec{\mu}_- \right)^T \Sigma_-^{-1} \left(\vec{x} - \vec{\mu}_- \right)
      \bigg) \Bigg)
  \end{align*}
\end{proposition}

\begin{proposition}
  Let $\{ P(X_+ \mid \vec{\mu}_+, \matr{I}), P(X_- \mid \vec{\mu}_-, \matr{I})
    \}$ be a binary multivariate GMM.
  The maximum-likelihood decision boundary is $\vec{x} = \frac{1}{2}(\vec{\mu}_+
    + \vec{\mu}_-)$.
  \begin{proof}
    \begin{equation*}
      \lambda(\vec{x}) = \exp \left(-\frac{1}{2}\left(
        \norm{\vec{x} - \vec{\mu}_+}^2 - \norm{\vec{x} - \vec{\mu}_-}^2
        \right)\right)
    \end{equation*}
    $\lambda(\vec{x}) = 1 \implies \norm{\vec{x} - \vec{\mu}_+}^2 = \norm{\vec{x} - \vec{\mu}_-}^2$,
    i.e., $\vec{x} = \frac{1}{2}(\vec{\mu}_+ + \vec{\mu}_-)$.
  \end{proof}
\end{proposition}

The maximum-likelihood decision boundary of a binary multivariate GMM for
uncorrelated features with unit variance is the same as the decision boundary
of the basic linear classifier.
In other words, the basic linear classifer is \textit{Bayes-optimal} under
these conditions.

\subsubsection{Distances and probabilities}

The normal distribution demonstrates the connection between the geometric and
probabilistic views of models.
Effectively, it translates distances into probabilities.

\begin{dfn}
  \label{dfn:9:normal-mahalanobis}
  The multivariate normal distribution
  (definition~\ref{dfn:9:multivariate-normal-distribution})
  can be expressed in terms of the Mahalanobis distance
  (definition~\ref{dfn:8:mahalanobis-distance}):
  \begin{equation}
    P(\vec{x} \mid \vec{\mu}, \matr{\Sigma})
    = \frac{1}{E_n} \exp \left(
    -\frac{1}{2} D_M (\vec{x}, \vec{\mu} \mid \matr{\Sigma})^2
    \right)
  \end{equation}
\end{dfn}

\begin{dfn}
  \label{dfn:9:normal-log-likelihood}
  The negative logarithm of the Gaussian likelihood is proportional to the
  squared Mahalanobis distance (definition~\ref{dfn:8:mahalanobis-distance}):
  \begin{equation}
    - \ln P(\vec{x} \mid \vec{\mu}, \matr{\Sigma})
    = \ln E_d + \frac{1}{2} D_M (\vec{x}, \vec{\mu} \mid \matr{\Sigma})^2
  \end{equation}
\end{dfn}

\begin{thm}
  Let $P(\vec{x} \mid \vec{\mu}, \matr{\Sigma})$ be a multivariate normal
  distribution.
  The maximum-likelihood estimate of $\vec{\mu}$ is the point that minimises the
  sum of squared Mahalanobis distances to the data points $X = \{ \vec{x}_i \mid
    i = 1 .. n \}$.

  \begin{proof}
    The maximum-likelihood estimate is the value of $\vec{\mu}$ that maximises
    the joint likelihood of $X$:
    \begin{equation}
      \vec{\hat{\mu}} = \argmax_{\vec{\mu}}
      P(X \mid \vec{\mu}, \matr{\Sigma})
    \end{equation}
    Assume that the data points
    are independently sampled from $P(\vec{x} \mid \vec{\mu}, \matr{\Sigma})$.
    Then, the joint likelihood is the product of the likelihoods of the data
    points:
    \begin{equation}
      P(X \mid \vec{\mu}, \matr{\Sigma})
      = \prod_{i = 1}^n P(\vec{x}_i \mid \vec{\mu}, \matr{\Sigma})
    \end{equation}
    By definitions~\ref{dfn:9:normal-mahalanobis} and
    \ref{dfn:9:normal-log-likelihood}:
    \begin{align}
      \vec{\hat{\mu}}
       & = \argmax_{\vec{\mu}}
      \prod_{i = 1}^n
      \frac{1}{E_n}
      \exp \left( -\frac{1}{2} D_M (\vec{x}, \vec{\mu} \mid \matr{\Sigma})^2 \right)
      \\
       & = \argmin_{\vec{\mu}}
      \sum_{i = 1}^n
      \left( \ln E_d + \frac{1}{2} D_M (\vec{x}, \vec{\mu} \mid \matr{\Sigma})^2 \right)
      \\
       & = \argmin_{\vec{\mu}} \sum_{i = 1}^n D_M (\vec{x}, \vec{\mu} \mid \matr{\Sigma})^2
    \end{align}
  \end{proof}
\end{thm}

\begin{dfn}
  \label{dfn:9:normal-euclidean}
  The standard normal distribution with $n = 2$
  (definition~\ref{dfn:9:multivariate-normal-distribution})
  can be expressed in terms of the Euclidean distance
  (definition~\ref{dfn:8:euclidean-distance}):
  \begin{equation}
    P(\vec{x} \mid \vec{0}, \matr{I})
    = \frac{1}{E_2} \exp\left( - \frac{1}{2} D_2 (\vec{x}, \vec{0})^2 \right)
  \end{equation}
\end{dfn}

\begin{thm}
  Let $P(\vec{x} \mid \vec{\mu}, \matr{I})$ be a multivariate normal
  distribution.
  The maximum-likelihood estimate of $\vec{\mu}$ is the point that minimises the
  sum of squared Euclidean distances to the data points $X = \{ \vec{x}_i \mid i
    = 1 .. n \}$.
\end{thm}

\subsubsection{Ordinary least-squares regression}

\begin{thm}
  Let $\hat{y}(x) = \alpha + \beta x$ be a univariate linear regression model and
  $X = \{ x_i \mid i = 1 .. n \},\ Y = \{ y_i \mid i = 1 .. n \}$ be a set of
  data points.
  If the noise is normally distributed, then the maximum-likelihood estimates of
  $\alpha$ and $\beta$ are equivalent to the ordinary least-squares solution.

  \begin{proof}
    Assume that $y_i$ is a noisy observation of $\hat{y}(x_i)$, i.e., $y_i =
      \hat{y}(x_i) + \epsilon_i$.
    If the noise is normally distributed, then the likelihood of $y_i$ is:
    \begin{equation}
      P(y_i \mid x_i, \alpha, \beta, \sigma) = \frac{1}{\sqrt{2 \pi} \sigma} \exp
      \left( - \frac{(y_i - \hat{y}(x_i))^2}{2 \sigma^2} \right)
    \end{equation}
    Assume that $\epsilon_i$ and $y_i$ are independent.
    Then, the joint likelihood of $Y$ is the product of the likelihoods of
    $y_i$:
    \begin{align}
      P(Y \mid X, \alpha, \beta, \sigma)
       & = \prod_{i = 1}^n P(y_i \mid x_i, \alpha, \beta, \sigma)
      \\
       & = \prod_{i = 1}^n \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(
      - \frac{(y_i - \hat{y}(x_i))^2}{2 \sigma^2}
      \right)
      \\
       & = \frac{1}{(2 \pi)^{n / 2}\,\sigma^n} \exp \left(
      -\frac{1}{2\sigma^2} \sum_{i = 1}^{n} (y_i - \hat{y}(x_i))^2
      \right)
    \end{align}
    Apply the negative logarithm and substitute $\hat{y}(x_i)$:
    \begin{equation}
      - \ln P(Y \mid X, \alpha, \beta, \sigma)
      = \frac{n}{2}\ln 2\pi + n\ln\sigma + \frac{1}{2\sigma^2} \sum_{i = 1}^{n} (y_i - (\alpha + \beta x_i))^2
    \end{equation}
    The negative log likelihood is minimised when its partial derivatives with
    respect to $\alpha$, $\beta$, and $\sigma^2$ are zero:
    \begin{equation}
      \begin{split}
        \frac{\partial}{\partial \alpha} - \ln P(Y \mid X, \alpha, \beta, \sigma)
        = \frac{1}{\sigma^2} \sum_{i = 1}^{n} (y_i - (\alpha + \beta x_i)) = 0 \\
        \implies \sum_{i = 1}^{n} (y_i - (\alpha + \beta x_i)) = 0
      \end{split}
    \end{equation}
    \begin{equation}
      \begin{split}
        \frac{\partial}{\partial \beta} - \ln P(Y \mid X, \alpha, \beta, \sigma)
        = \frac{1}{\sigma^2} \sum_{i = 1}^{n} (y_i - (\alpha + \beta x_i)) x_i = 0 \\
        \implies \sum_{i = 1}^{n} (y_i - (\alpha + \beta x_i)) x_i = 0
      \end{split}
    \end{equation}
    \begin{equation}
      \begin{split}
        \frac{\partial}{\partial \sigma^2} - \ln P(Y \mid X, \alpha, \beta, \sigma)
        = \frac{n}{2\sigma^2} - \frac{1}{2\sigma^4} \sum_{i = 1}^{n} (y_i - (\alpha + \beta x_i))^2 = 0 \\
        \implies \sum_{i = 1}^{n} (y_i - (\alpha + \beta x_i))^2 = n\sigma^2
      \end{split}
    \end{equation}
  \end{proof}
\end{thm}

\subsection{Naïve Bayes}

In the context of classification, it is assumed that a distribution that models
the data $X$ depends on the class $Y$.
The greater the differences between the distributions for the different
classes, the better the model can discriminate between them.
Several decision rules can be applied:
\begin{itemize}
  \item Maximum likelihood (ML):
        \begin{equation}
          \hat{y} = \argmax_y P(X = x \mid Y = y)
        \end{equation}
  \item Maximum a posteriori (MAP):
        \begin{equation}
          \hat{y} = \argmax_y P(X = x \mid Y = y) P(Y = y)
        \end{equation}
  \item Recalibrated likelihood:
        \begin{equation}
          \hat{y} = \argmax_y w_y P(X = x \mid Y = y)
        \end{equation}
\end{itemize}
ML and MAP are equivalent if the prior distribution of $Y$ is uniform.
The recalibrated likelihood generalises ML and MAP by a set of weights $w_y$.
With uncalibrated probability estimates, the recalibrated likelihood is needed.

\subsection{TODO}

\begin{itemize}
  \item Categorical variables
  \item Categorical probability distributions
  \item Naïve Bayes
  \item Logistic regression
\end{itemize}
