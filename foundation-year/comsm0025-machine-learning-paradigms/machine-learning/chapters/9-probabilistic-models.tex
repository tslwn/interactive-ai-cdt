\section{Probabilistic models}
\label{sec:9:probabilistic-models}

Recall that:
\begin{itemize}
  \item $P(Y \mid X)$ is the posterior probability distribution of $Y$ given $X$;
  \item $P(Y, X)$ is the joint probability distribution of $Y$ and $X$;
  \item $P(X \mid Y)$ is the likelihood function; and
  \item $P(X)$ is the prior distribution of $X$.
\end{itemize}

\paragraph{Discriminative and generative models}

\begin{itemize}
  \item A \textit{discriminative} model describes the posterior distribution of
        the target given the input.
        It does not describe the prior distribution of the input.
  \item A \textit{generative} model describes a joint distribution of the target
        and input.
        If the prior distribution of the target can be estimated, then it can
        be described by a likelihood function.
        It can be used to generate data by sampling from the joint distribution.
\end{itemize}

Generative models can do more than discriminative models.
However, joint distributions are harder to learn than conditional distributions
like the posterior distribution because they are described by more probability
values (parameters).
This may be handled by simplifying assumptions like independence but they are
not always appropriate.

\paragraph{Uncertainty}

A probabilistic view treats learning as a procedure that reduces uncertainty.

\subsection{Normal distributions}

\paragraph{Univariate}

\begin{dfn}
  [Univariate normal distribution]
  \label{dfn:9:univariate-normal-distribution}
  \begin{align}
    P(x \mid \mu, \sigma)
     & = \frac{1}{\sqrt{2 \pi} \sigma} \exp \left( - \frac{(x - \mu)^2}{2 \sigma^2} \right) \\
     & = \frac{1}{E} \exp \left( - \frac{z^2}{2} \right) ,\
    E = \sqrt{2 \pi} \sigma
  \end{align}
  \begin{itemize}
    \item $\mu \in \mathbb{R}$ is the mean;
    \item $\sigma \in \mathbb{R}$ is the standard deviation; and
    \item $z = \frac{x - \mu}{\sigma}$ is the $z$-score.
  \end{itemize}
\end{dfn}
If $\mu = 0$ and $\sigma = 1$, then it is the \textit{standard} univariate
normal distribution:
\begin{equation}
  P(x \mid \mu = 0, \sigma = 1)
  = \frac{1}{\sqrt{2 \pi}} \exp \left( - \frac{x^2}{2} \right)
\end{equation}

\paragraph{Multivariate}

\begin{dfn}
  [Multivariate normal distribution]
  \label{dfn:9:multivariate-normal-distribution}
  \begin{equation}
    \begin{split}
      P(\vec{x} \mid \vec{\mu}, \matr{\Sigma})
      = \frac{1}{E_n} \exp \left(
      - \frac{1}{2} (\vec{x} - \vec{\mu})^T \matr{\Sigma}^{-1} (\vec{x} - \vec{\mu})
      \right) ,\\
      E_n = (2 \pi)^{n/2} \sqrt{\det \matr{\Sigma}}
    \end{split}
  \end{equation}
  \begin{itemize}
    \item $\vec{\mu} \in \mathbb{R}^n$ is the mean; and
    \item $\matr{\Sigma} \in \mathbb{R}^{n \times n}$ is the covariance matrix.
  \end{itemize}
\end{dfn}
If $\vec{\mu} = \vec{0}$ and $\matr{\Sigma} = \matr{I}$, then it is the
\textit{standard} multivariate normal distribution:
\begin{equation}
  P(\vec{x} \mid \vec{\mu} = \vec{0}, \matr{\Sigma} = \matr{I})
  = \frac{1}{(2 \pi)^{n / 2}} \exp \left( - \frac{\vec{x} \cdot \vec{x}}{2} \right)
\end{equation}

\subsubsection{Gaussian mixture models}

A Gaussian mixture model is a mixture of $k$ Gaussian distributions.
For $k = 2$, i.e., binary classification,
$X = \{ x_i \mid i = 1 .. n \} = X_+ \cup X_-$.

\paragraph{Univariate}

In the univariate case, the likelihood ratio is:
\begin{equation*}
  \frac{P(X_+)}{P(X_-)}
  = \frac{\sigma_-}{\sigma_+} \exp \left(
  - \frac{1}{2} \left(
    \left( \frac{x - \mu_+}{\sigma_+} \right)^2
    - \left( \frac{x - \mu_-}{\sigma_-} \right)^2
    \right)
  \right)
\end{equation*}

If $\sigma_+ = \sigma_- = \sigma$, then the likelihood ratio is:
\begin{equation*}
  \exp (\gamma (x - \mu)) ,\
  \gamma = \frac{\mu_+ - \mu_-}{\sigma^2} ,\
  \mu = \frac{\mu_+ + \mu_-}{2}
\end{equation*}
and the maximum-likelihood decision threshold, i.e., the value of $x$ such
that the likelihood ratio is 1, is $\mu$.

If the standard deviations of the two Gaussian distributions are different, then
there are two decision boundaries and a non-contiguous decision region for one
of the classes.

\paragraph{Multivariate}

In the multivariate case, the likelihood ratio is:
\begin{equation*}
  \sqrt{\frac{\det\Sigma_+}{\det\Sigma_-}}
  \exp \left(-\frac{1}{2} \left(
    \left(\vec{x} - \vec{\mu}_+ \right)^T \Sigma_+^{-1}
    \left(\vec{x} - \vec{\mu}_+ \right) -
    \left(\vec{x} - \vec{\mu}_- \right)^T \Sigma_-^{-1}
    \left(\vec{x} - \vec{\mu}_- \right)
    \right) \right)
\end{equation*}

If $\matr{\Sigma}_+ = \matr{\Sigma}_- = \matr{I}$, i.e., for each class, the
features are uncorrelated and have unit variance, then the likelihood ratio is:
\begin{equation*}
  \exp \left(
  -\frac{1}{2} \left(
    \norm{\vec{x} - \vec{\mu}_+}^2 - \norm{\vec{x} - \vec{\mu}_-}^2
    \right)\right)
\end{equation*}
and the maximum-likehood decision boundary, i.e., the values of $\vec{x}$ such
that the likelihood ratio is 1, is the hyperplane equidistant from $\vec{\mu}_+$
and $\vec{\mu}_-$.

This is the same as the decision boundary for the basic linear classifier.
In other words, for uncorrelated Gaussian features with unit variance, the basic
linear classifier is \textit{Bayes-optimal}.

\subsubsection{Distances and probabilities}

The normal distribution demonstrates the connection between the geometric and
probabilistic views of models.
Effectively, it translates distances into probabilities.

\begin{dfn}
  \label{dfn:9:normal-mahalanobis}
  The multivariate normal distribution
  (definition~\ref{dfn:9:multivariate-normal-distribution})
  can be expressed in terms of the Mahalanobis distance
  (definition~\ref{dfn:8:mahalanobis-distance}):
  \begin{equation}
    P(\vec{x} \mid \vec{\mu}, \matr{\Sigma})
    = \frac{1}{E_n} \exp \left(
    -\frac{1}{2} D_M (\vec{x}, \vec{\mu} \mid \matr{\Sigma})^2
    \right)
  \end{equation}
\end{dfn}

\begin{dfn}
  \label{dfn:9:normal-log-likelihood}
  The negative logarithm of the Gaussian likelihood is proportional to the
  squared Mahalanobis distance (definition~\ref{dfn:8:mahalanobis-distance}):
  \begin{equation}
    - \ln P(\vec{x} \mid \vec{\mu}, \matr{\Sigma})
    = \ln E_d + \frac{1}{2} D_M (\vec{x}, \vec{\mu} \mid \matr{\Sigma})^2
  \end{equation}
\end{dfn}

\begin{thm}
  Let $P(\vec{x} \mid \vec{\mu}, \matr{\Sigma})$ be a multivariate normal
  distribution.
  The maximum-likelihood estimate of $\vec{\mu}$ is the point that minimises
  the sum of squared Mahalanobis distances to the data points
  $X = \{ \vec{x}_i \mid i = 1 .. n \}$.

  \begin{proof}
    The maximum-likelihood estimate is the value of $\vec{\mu}$ that maximises
    the joint likelihood of $X$:
    \begin{equation}
      \vec{\hat{\mu}} = \argmax_{\vec{\mu}} P(X \mid \vec{\mu}, \matr{\Sigma})
    \end{equation}
    Assume that the data points are independently sampled from
    $P(\vec{x} \mid \vec{\mu}, \matr{\Sigma})$.
    Then, the joint likelihood is the product of the likelihoods of the data
    points:
    \begin{equation}
      P(X \mid \vec{\mu}, \matr{\Sigma})
      = \prod_{i = 1}^n P(\vec{x}_i \mid \vec{\mu}, \matr{\Sigma})
    \end{equation}
    By definitions~\ref{dfn:9:normal-mahalanobis} and
    \ref{dfn:9:normal-log-likelihood}:
    \begin{align}
      \vec{\hat{\mu}}
       & = \argmax_{\vec{\mu}}
      \prod_{i = 1}^n
      \frac{1}{E_n}
      \exp \left( -\frac{1}{2} D_M (\vec{x}, \vec{\mu} \mid \matr{\Sigma})^2 \right)        \\
       & = \argmin_{\vec{\mu}}
      \sum_{i = 1}^n
      \left( \ln E_d + \frac{1}{2} D_M (\vec{x}, \vec{\mu} \mid \matr{\Sigma})^2 \right)    \\
       & = \argmin_{\vec{\mu}} \sum_{i = 1}^n D_M (\vec{x}, \vec{\mu} \mid \matr{\Sigma})^2
    \end{align}
  \end{proof}
\end{thm}

\begin{dfn}
  \label{dfn:9:normal-euclidean}
  The standard normal distribution with $n = 2$
  (definition~\ref{dfn:9:multivariate-normal-distribution})
  can be expressed in terms of the Euclidean distance
  (definition~\ref{dfn:8:euclidean-distance}):
  \begin{equation}
    P(\vec{x} \mid \vec{0}, \matr{I})
    = \frac{1}{E_2} \exp\left( - \frac{1}{2} D_2 (\vec{x}, \vec{0})^2 \right)
  \end{equation}
\end{dfn}

\begin{thm}
  Let $P(\vec{x} \mid \vec{\mu}, \matr{I})$ be a multivariate normal
  distribution.
  The maximum-likelihood estimate of $\vec{\mu}$ is the point that minimises
  the sum of squared Euclidean distances to the data points
  $X = \{ \vec{x}_i \mid i = 1 .. n \}$.
\end{thm}

\subsubsection{Ordinary least-squares regression}

\begin{thm}
  Let $\hat{y}(x) = \alpha + \beta x$ be a univariate linear regression model
  and $X = \{ x_i \mid i = 1 .. n \},\ Y = \{ y_i \mid i = 1 .. n \}$ be a set of
  data points.
  If the noise is normally distributed, then the maximum-likelihood estimates of
  $\alpha$ and $\beta$ are equivalent to the ordinary least-squares solution.

  \begin{proof}
    Assume that $y_i$ is a noisy observation of $\hat{y}(x_i)$, i.e.,
    $y_i = \hat{y}(x_i) + \epsilon_i$.
    If the noise is normally distributed, then the likelihood of $y_i$ is:
    \begin{equation}
      P(y_i \mid x_i, \alpha, \beta, \sigma)
      = \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(
      - \frac{(y_i - \hat{y}(x_i))^2}{2 \sigma^2}
      \right)
    \end{equation}
    Assume that $\epsilon_i$ and $y_i$ are independent.
    Then, the joint likelihood of $Y$ is the product of the likelihoods of
    $y_i$:
    \begin{align}
      P(Y \mid X, \alpha, \beta, \sigma)
       & = \prod_{i = 1}^n P(y_i \mid x_i, \alpha, \beta, \sigma)    \\
       & = \prod_{i = 1}^n \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(
      - \frac{(y_i - \hat{y}(x_i))^2}{2 \sigma^2}
      \right)                                                        \\
       & = \frac{1}{(2 \pi)^{n / 2}\,\sigma^n} \exp \left(
      -\frac{1}{2\sigma^2} \sum_{i = 1}^{n} (y_i - \hat{y}(x_i))^2
      \right)
    \end{align}
    Apply the negative logarithm and substitute $\hat{y}(x_i)$:
    \begin{equation}
      - \ln P(Y \mid X, \alpha, \beta, \sigma)
      = \frac{n}{2}\ln 2\pi + n\ln\sigma + \frac{1}{2\sigma^2} \sum_{i = 1}^{n} (y_i - (\alpha + \beta x_i))^2
    \end{equation}
    The negative log likelihood is minimised when its partial derivatives with
    respect to $\alpha$, $\beta$, and $\sigma^2$ are zero:
    \begin{equation}
      \begin{split}
        \frac{\partial}{\partial \alpha} - \ln P(Y \mid X, \alpha, \beta, \sigma)
        = \frac{1}{\sigma^2} \sum_{i = 1}^{n} (y_i - (\alpha + \beta x_i)) = 0 \\
        \implies \sum_{i = 1}^{n} (y_i - (\alpha + \beta x_i)) = 0
      \end{split}
    \end{equation}
    \begin{equation}
      \begin{split}
        \frac{\partial}{\partial \beta} - \ln P(Y \mid X, \alpha, \beta, \sigma)
        = \frac{1}{\sigma^2} \sum_{i = 1}^{n} (y_i - (\alpha + \beta x_i)) x_i = 0 \\
        \implies \sum_{i = 1}^{n} (y_i - (\alpha + \beta x_i)) x_i = 0
      \end{split}
    \end{equation}
    \begin{equation}
      \begin{split}
        \frac{\partial}{\partial \sigma^2} - \ln P(Y \mid X, \alpha, \beta, \sigma)
        = \frac{n}{2\sigma^2} - \frac{1}{2\sigma^4} \sum_{i = 1}^{n} (y_i - (\alpha + \beta x_i))^2 = 0 \\
        \implies \sum_{i = 1}^{n} (y_i - (\alpha + \beta x_i))^2 = n\sigma^2
      \end{split}
    \end{equation}
  \end{proof}
\end{thm}

\subsection{Naïve Bayes}

In the context of classification, it is assumed that a distribution that models
the data $X$ depends on the class $Y$.
The greater the differences between the distributions for the different classes,
the better the model can discriminate between them.
Several decision rules can be applied:
\begin{itemize}
  \item Maximum likelihood (ML):
        \begin{equation}
          \hat{y} = \argmax_y P(X = x \mid Y = y)
        \end{equation}
  \item Maximum a posteriori (MAP):
        \begin{equation}
          \hat{y} = \argmax_y P(X = x \mid Y = y) P(Y = y)
        \end{equation}
  \item Recalibrated likelihood:
        \begin{equation}
          \hat{y} = \argmax_y w_y P(X = x \mid Y = y)
        \end{equation}
\end{itemize}
ML and MAP are equivalent if the prior distribution of $Y$ is uniform.
The recalibrated likelihood generalises ML and MAP by a set of weights $w_y$.
With uncalibrated probability estimates, the recalibrated likelihood is needed.

\subsection{TODO}

\begin{itemize}
  \item Categorical variables
  \item Categorical probability distributions
  \item Naïve Bayes
  \item Logistic regression
\end{itemize}