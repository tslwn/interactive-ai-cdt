\section{Distance-based models}

A distance-based model is generally comprised of:
\begin{itemize}
  \item a distance metric
        (section~\ref{sec:8:distance-metrics});
  \item a set of exemplars
        (paragraph~\ref{par:8:centroids-and-medoids}); and
  \item a distance-based decision rule
        (e.g., paragraph~\ref{par:8:binary-linear-classifiers}).
\end{itemize}

\subsection{Distance metrics}
\label{sec:8:distance-metrics}

\paragraph{Metrics}

\begin{dfn}
  [Metric]
  A metric is a function $d : M \times M \rightarrow \mathbb{R}$, where $M$ is a
  set of points, such that:
  \begin{enumerate}
    \item $d(x, x) = 0 \ \forall\ x \in M$
          (the distance from a point to itself is zero)
    \item $d(x, y) > 0 \ \forall\ x, y \in M, x \neq y$
          (positivity)
    \item $d(x, y) = d(y, x) \ \forall\ x, y \in M$
          (symmetry)
    \item $d(x, z) \leq d(x, y) + d(y, z) \ \forall\ x, y, z \in M$
          (triangle inequality)
  \end{enumerate}
\end{dfn}

\begin{dfn}
  [Pseudo-metric]
  A pseudo-metric is a metric where the condition of positivity is replaced by
  non-negativity, i.e., $d(x, y) \geq 0 \ \forall\ x, y \in M$.
\end{dfn}

\paragraph{Norms}

\begin{dfn}
  [$p$-norm, $L_{p}$ norm]
  The $p$-norm of a vector $\vec{x} \in \mathbb{R}^n$ is:
  \begin{equation}
    \norm{\vec{x}}_{p} = \left( \sum_{i = 1}^{n} |x_i|^{p} \right)^{\frac{1}{p}}
  \end{equation}
\end{dfn}

\begin{dfn}
  [0-``norm", $L_{0}$ ``norm"]
  The 0-``norm" of a vector $\vec{x} \in \mathbb{R}^n$ is the number of non-zero
  elements in $\vec{x}$:
  \begin{equation}
    \norm{\vec{x}}_{0} = \sum_{i = 1}^{n} |x_i|^{0}
  \end{equation}
\end{dfn}

The 0-``norm" is not a norm because it is not homogeneous.

\paragraph{Distances}

\begin{dfn}
  [Minkowski distance]
  The Minkowski distance of order $p \in \mathbb{N}_1$ between two vectors
  $\vec{x}, \vec{y} \in \mathbb{R}^n$ is:
  \begin{equation}
    D_{p}(\vec{x}, \vec{y})
    = \left( \sum_{i = 1}^{n} |x_i - y_i|^{p} \right)^{\frac{1}{p}}
    = \norm{\vec{x} - \vec{y}}_{p}
  \end{equation}
\end{dfn}

\begin{dfn}
  [Manhattan distance]
  The Manhattan distance between two vectors $\vec{x}, \vec{y} \in \mathbb{R}^n$
  is the Minkowski distance of order $p = 1$:
  \begin{equation}
    D_{1}(\vec{x}, \vec{y})
    = \sum_{i = 1}^{n} |x_i - y_i|
  \end{equation}
\end{dfn}

\begin{dfn}
  [Euclidean distance]
  The Euclidean distance between two vectors $\vec{x}, \vec{y} \in \mathbb{R}^n$
  is the Minkowski distance of order $p = 2$:
  \begin{equation}
    D_{2}(\vec{x}, \vec{y})
    = \sqrt{ \sum_{i = 1}^{n} (x_i - y_i)^{2} }
  \end{equation}
\end{dfn}

\begin{dfn}
  [Chebyshev distance]
  The Chebyshev distance between two vectors $\vec{x}, \vec{y} \in \mathbb{R}^n$
  is the Minkowski distance of order $p \rightarrow \infty$:
  \begin{equation}
    D_{\infty}(\vec{x}, \vec{y})
    = \lim_{p \rightarrow \infty} \left( \sum_{i = 1}^{n} |x_i - y_i|^{p} \right)^{\frac{1}{p}}
    = \max_{i = 1}^{n} |x_i - y_i|
  \end{equation}
\end{dfn}

Minkowski distances are translationally invariant but not scale-invariant.
Euclidean distance is the only Minkowski distance that is rotationally invariant.

\begin{dfn}
  [Hamming distance]
  The Hamming distance between two binary strings $\vec{x}, \vec{y}$ of length
  $n$ is the number of bits in which they differ:
  \begin{equation}
    D_{0}(\vec{x}, \vec{y})
    = \sum_{i = 1}^{n} |x_i - y_i|^{0}
    = \sum_{i = 1}^{n} \mathbb{I}(x_i \neq y_i)
  \end{equation}
\end{dfn}

The edit or \textit{Levenshtein distance} generalises the Hamming distance to
non-binary strings of different lengths.

\begin{dfn}
  [Mahalanobis distance]
  The Mahalanobis distance between two vectors $\vec{x}, \vec{y} \in \mathbb{R}^n$,
  where $\matr{\Sigma}$ is the covariance matrix, is:
  \begin{equation}
    D_{M}(\vec{x}, \vec{y} \mid \matr{\Sigma})
    = \sqrt{ (\vec{x} - \vec{y})^{\top} \Sigma^{-1} (\vec{x} - \vec{y}) }
  \end{equation}
\end{dfn}

Euclidean distance is the Mahalanobis distance where the covariance matrix is
the identity matrix.

\subsection{Neighbours and exemplars}

\paragraph{Means and medians}

Minimising the sum of squared Euclidean distances is equivalent to minimising
the \textit{average} squared Euclidean distance.

\begin{thm}
  [Arithmetic mean minimises squared Euclidean distance]
  The arithmetic mean $\vec{\mu}$ of a set of points $X \in \mathbb{R}^n$ is the
  point with the minimum sum of squared Euclidean distances to the points in $X$:
  \begin{equation}
    \argmin_{\vec{y}} \sum_{\vec{x} \in X} \norm{\vec{x} - \vec{y}}_{2}^{2} = \vec{\mu}
  \end{equation}
  \begin{proof}
    The gradient of the sum of squared Euclidean distances is:
    \begin{align*}
      \nabla_{\vec{y}} \sum_{\vec{x} \in X} \norm{\vec{x} - \vec{y}}_{2}^{2}
       & = - 2 \sum_{\vec{x} \in X} (\vec{x} - \vec{y})          \\
       & = - 2 \sum_{\vec{x} \in X} \vec{x} + 2 \card{X} \vec{y}
    \end{align*}
    If the gradient is the zero vector, then:
    \begin{equation*}
      \vec{y} = \frac{1}{\card{X}} \sum_{\vec{x} \in X} \vec{x} = \vec{\mu}
    \end{equation*}
  \end{proof}
\end{thm}

The \textit{geometric median} minimises the sum of Euclidean distances.
However, there is no closed-form expression for the geometric median of
multivariate data.

\paragraph{Centroids and medoids}
\label{par:8:centroids-and-medoids}

A \textit{centroid} is an exemplar that is not necessarily an instance, whereas
a \textit{medoid} must be an instance.
An algorithm to find the medoid of a set of $n$ instances has time complexity
$O(n^2)$.
This is because the distance between every pair of instances must be computed.

\paragraph{Binary linear classifiers}
\label{par:8:binary-linear-classifiers}

A binary linear classifier finds the exemplars that minimise the sum of squared
Euclidean distances to the instances in each class.
Its decision boundary is the perpendicular bisector of the line segment that
connects the exemplars.
Alternatively, it applies the \textit{decision rule} that an instance belongs to
the class with the nearest exemplar.

\paragraph{Multi-class linear classifiers}

A distance-based interpretation of the binary linear classifer generalises to
$k > 2$ classes.
With $k$ exemplars, each decision region is bounded by $k - 1$ line segments.
Dependent on the distance metric, some decision regions become closed cells as
the number of exemplars increases.
This is called \textit{Voronoi tesselation}.
Generally, the number of exemplars is greater than the number of classes.