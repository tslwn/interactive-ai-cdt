\section{Distance-based models}
\label{sec:8:distance-based-models}

A distance-based model is generally comprised of:
\begin{itemize}
  \item a distance metric
        (section~\ref{sec:8:distance-metrics});
  \item a set of exemplars
        (section~\ref{par:8:centroids-and-medoids}); and
  \item a distance-based decision rule
        (e.g., section~\ref{par:8:binary-linear-classifiers}).
\end{itemize}

\subsection{Distance metrics}
\label{sec:8:distance-metrics}

\begin{dfn}
  [Metric]
  A metric is a function $d : M \times M \rightarrow \mathbb{R}$, where $M$ is a
  set of points, such that:
  \begin{enumerate}
    \item $d(x, x) = 0 \ \forall\ x \in M$
          (the distance from a point to itself is zero)
    \item $d(x, y) > 0 \ \forall\ x, y \in M, x \neq y$
          (positivity)
    \item $d(x, y) = d(y, x) \ \forall\ x, y \in M$
          (symmetry)
    \item $d(x, z) \leq d(x, y) + d(y, z) \ \forall\ x, y, z \in M$
          (triangle inequality)
  \end{enumerate}
\end{dfn}

\begin{dfn}
  [Pseudo-metric]
  A pseudo-metric is a metric where the condition of positivity is replaced by
  non-negativity, i.e., $d(x, y) \geq 0 \ \forall\ x, y \in M$.
\end{dfn}

\subsubsection{Norms}

\begin{dfn}
  [$p$-norm, $L_{p}$ norm]
  The $p$-norm of a vector $\vec{x} \in \mathbb{R}^n$ is:
  \begin{equation}
    \norm{\vec{x}}_{p} = \left( \sum_{i = 1}^{n} |x_i|^{p} \right)^{\frac{1}{p}}
  \end{equation}
\end{dfn}

\begin{dfn}
  [0-``norm", $L_{0}$ ``norm"]
  The 0-``norm" of a vector $\vec{x} \in \mathbb{R}^n$ is the number of non-zero
  elements in $\vec{x}$:
  \begin{equation}
    \norm{\vec{x}}_{0} = \sum_{i = 1}^{n} |x_i|^{0}
  \end{equation}
\end{dfn}

The 0-``norm" is not a norm because it is not \textit{homogeneous}, i.e.,
$f(ax) \neq a f(x) \ \forall\ a \in \mathbb{R}, x \in X$.

\subsubsection{Distances}

\begin{dfn}
  [Minkowski distance]
  The Minkowski distance of order $p \in \mathbb{N}_1$ between two vectors
  $\vec{x}, \vec{y} \in \mathbb{R}^n$ is:
  \begin{equation}
    D_{p}(\vec{x}, \vec{y})
    = \left( \sum_{i = 1}^{n} |x_i - y_i|^{p} \right)^{\frac{1}{p}}
    = \norm{\vec{x} - \vec{y}}_{p}
  \end{equation}
\end{dfn}

\begin{dfn}
  [Manhattan distance]
  The Manhattan distance between two vectors $\vec{x}, \vec{y} \in \mathbb{R}^n$
  is the Minkowski distance of order $p = 1$:
  \begin{equation}
    D_{1}(\vec{x}, \vec{y})
    = \sum_{i = 1}^{n} |x_i - y_i|
  \end{equation}
\end{dfn}

\begin{dfn}
  [Euclidean distance]
  The Euclidean distance between two vectors $\vec{x}, \vec{y} \in \mathbb{R}^n$
  is the Minkowski distance of order $p = 2$:
  \begin{equation}
    D_{2}(\vec{x}, \vec{y})
    = \sqrt{ \sum_{i = 1}^{n} (x_i - y_i)^{2} }
  \end{equation}
\end{dfn}

\begin{dfn}
  [Chebyshev distance]
  The Chebyshev distance between two vectors $\vec{x}, \vec{y} \in \mathbb{R}^n$
  is the Minkowski distance of order $p \rightarrow \infty$:
  \begin{equation}
    D_{\infty}(\vec{x}, \vec{y})
    = \lim_{p \rightarrow \infty} \left( \sum_{i = 1}^{n} |x_i - y_i|^{p} \right)^{\frac{1}{p}}
    = \max_{i = 1}^{n} |x_i - y_i|
  \end{equation}
\end{dfn}

Minkowski distances are translationally invariant but not scale-invariant.
Euclidean distance is the only Minkowski distance that is rotationally invariant.

\begin{dfn}
  [Hamming distance]
  The Hamming distance between two binary strings $\vec{x}, \vec{y}$ of length
  $n$ is the number of bits in which they differ:
  \begin{equation}
    D_{0}(\vec{x}, \vec{y})
    = \sum_{i = 1}^{n} |x_i - y_i|^{0}
    = \sum_{i = 1}^{n} \mathbb{I}(x_i \neq y_i)
  \end{equation}
\end{dfn}

The edit or \textit{Levenshtein distance} generalises the Hamming distance to
non-binary strings of different lengths.

\begin{dfn}
  [Mahalanobis distance]
  The Mahalanobis distance between two vectors $\vec{x}, \vec{y} \in \mathbb{R}^n$,
  where $\matr{\Sigma}$ is the covariance matrix, is:
  \begin{equation}
    D_{M}(\vec{x}, \vec{y} \mid \matr{\Sigma})
    = \sqrt{ (\vec{x} - \vec{y})^{\top} \Sigma^{-1} (\vec{x} - \vec{y}) }
  \end{equation}
\end{dfn}

Euclidean distance is the Mahalanobis distance where the covariance matrix is
the identity matrix.

\subsection{Neighbours and exemplars}

\subsubsection{Means and medians}

Minimising the sum of squared Euclidean distances is equivalent to minimising
the \textit{average} squared Euclidean distance.

\begin{thm}
  [Arithmetic mean minimises squared Euclidean distance]
  The arithmetic mean $\vec{\mu}$ of a set of points $X \in \mathbb{R}^n$ is the
  point with the minimum sum of squared Euclidean distances to the points in $X$:
  \begin{equation}
    \argmin_{\vec{y}} \sum_{\vec{x} \in X} \norm{\vec{x} - \vec{y}}_{2}^{2} = \vec{\mu}
  \end{equation}
  \begin{proof}
    The gradient of the sum of squared Euclidean distances is:
    \begin{align*}
      \nabla_{\vec{y}} \sum_{\vec{x} \in X} \norm{\vec{x} - \vec{y}}_{2}^{2}
       & = - 2 \sum_{\vec{x} \in X} (\vec{x} - \vec{y})          \\
       & = - 2 \sum_{\vec{x} \in X} \vec{x} + 2 \card{X} \vec{y}
    \end{align*}
    If the gradient is the zero vector, then:
    \begin{equation*}
      \vec{y} = \frac{1}{\card{X}} \sum_{\vec{x} \in X} \vec{x} = \vec{\mu}
    \end{equation*}
  \end{proof}
\end{thm}

The \textit{geometric median} minimises the sum of Euclidean distances.
However, there is no closed-form expression for the geometric median of
multivariate data.

\subsubsection{Centroids and medoids}
\label{par:8:centroids-and-medoids}

A \textit{centroid} is an exemplar that is not necessarily an instance, whereas
a \textit{medoid} must be an instance.
An algorithm to find the medoid of a set of $n$ instances has time complexity
$O(n^2)$.
This is because the distance between every pair of instances must be computed.

\subsubsection{Binary linear classifiers}
\label{par:8:binary-linear-classifiers}

A binary linear classifier finds the exemplars that minimise the sum of squared
Euclidean distances to the instances in each class.
Its decision boundary is the perpendicular bisector of the line segment that
connects the exemplars.
Alternatively, it applies the \textit{decision rule} that an instance belongs to
the class with the nearest exemplar.

\subsubsection{Multi-class linear classifiers}
\label{par:8:multi-class-linear-classifiers}

A distance-based interpretation of the binary linear classifer generalises to
$k > 2$ classes.
With $k$ exemplars, each decision region is bounded by $k - 1$ line segments.
Dependent on the distance metric, some decision regions become closed cells as
the number of exemplars increases.
This is called \textit{Voronoi tesselation}.
Generally, the number of exemplars is greater than the number of classes.

\subsection{Nearest-neighbour classifiers}

Paragraph~\ref{par:8:multi-class-linear-classifiers} generalised the binary
linear classifier to $k > 2$ classes.
The \textit{nearest-neighbour classifier} is simpler: it takes each instance to
be an exemplar.
Its decision regions are sets of Voronoi cells (because adjacent cells may have
the same class).

\subsubsection{Classifier properties}

The nearest-neighbour classifier has low bias and high variance.
For $n$ instances, it is `trained' in $O(n)$ time.
However, it may also take $O(n)$ time to classify a new instance.
This is because the new instance must be compared with every training instance.
The classification time can be decreased at the expense of the training time by
choosing the data structure that stores the instances.

\subsubsection{Dimensionality}

High-dimensional instance spaces are sparse, i.e., the distance between any two
instances is large.
The effective dimensionality may be smaller: some dimensions may be irrelevant
or the instances may lie on a lower-dimensional manifold.
The dimensionality of the instance space can be reduced by \textit{feature selection}
or techniques such as \textit{principal component analysis} (PCA) before
applying a distance-based model.

\subsubsection{\textit{k}-nearest neighbours}

If there is a way to aggregate over exemplars, then $k$ nearest neighbours can
be used.
An example of a decision rule for a $k$-nearest-neighbour classifier is to
predict the majority class of the $k$ nearest exemplars.

The model's properties depend on the choice of $k$: as it increases, the
the refinement first increases and then decreases, the bias increases, and the
variance decreases.
The dependence on $k$ can be lessened by applying \textit{distance weighting}
to the exemplars.

\subsubsection{Regression}

Nearest-neighbour approaches are agnostic with respect to the type of the target
variable and can be applied to regression problems.
With $k$-nearest-neighbours, the predicted value is typically the mean of the
$k$ nearest exemplars, which may be distance-weighted.

\subsection{Clustering}
\label{sec:8:clustering}

For distance-based models, unsupervised learning generally refers to clustering.
A predictive distance-based clustering method has the same elements as a
distance-based classifier (section~\ref{sec:8:distance-based-models}).
Instead of an explicit target variable, the distance metric is taken to
represent the learning target.
Therefore, the aim is to find \textit{compact} clusters with respect to the
distance metric.

\begin{dfn}
  [Within-cluster scatter matrix]
  Let $X = \bigcup_{i = 1}^{k} X_i$ be a set of instances partitioned into $k$ classes.
  The within-cluster scatter matrix $\matr{S}_i$ is the scatter matrix of $X_i$.
\end{dfn}

\begin{dfn}
  [Between-cluster scatter matrix]
  Let $X = \bigcup_{i = 1}^{k} X_i$ be a set of instances partitioned into $k$ classes.
  The between-cluster scatter matrix $\matr{B}$ is the scatter matrix of $X$
  where each instance is replaced by its centroid $\vec{\mu}_i$.
\end{dfn}

\begin{thm}
  [Relation of within- and between-cluster scatter matrices]
  \begin{align}
    \matr{S}
     & = \sum_{i = 1}^{k} \matr{S}_i + \matr{B} \\
    \Tr \matr{S}
     & = \sum_{i = 1}^{k} \Tr \matr{S}_i +
    \sum_{i = 1}^{k} \card{X_i} \norm{\vec{\mu}_j - \vec{\mu}}^2
  \end{align}
\end{thm}

Minimising the total within-cluster scatter is equivalent to maximising the
scatter of the centroids, weighted by the number of instances in each class.

\subsubsection{\textit{k}-means}

The \textit{$k$-means} problem is to find the partition of $X$ that minimises
the total within-cluster scatter (section~\ref{sec:8:clustering}).
The $k$-means problem is NP-complete, i.e., there is no efficient way to find
the global minimum.
The typical heuristic algorithm is called $k$-means or
\textit{Lloyd's algorithm}.
It iterates between partitioning the data with the nearest-centroid decision
rule and recomputing the centroids from the partition.

An iteration of the $k$-means algorithm cannot decrease the total within-cluster
scatter, so it reaches a \textit{stationary point} (a local minimum).
It converges to a stationary point in a finite number of iterations but there is
no way to know whether it is optimal (the global minimum).
Therefore, it is recommended to run $k$-means multiple times and choose the
best solution.

\subsubsection{\textit{k}-medoids}

The $k$-medoids algorithm uses medoids instead of centroids.
An alternative is the \textit{partitioning around medoids} (PAM) algorithm,
which swaps medoids with other instances in the class.
The clustering quality is the distance between the medoids and the instances in
the class.
Each iteration takes $O(k(n - k)^2)$ time.

\subsubsection{Shape}

Methods that represent clusters only by exemplars disregard the shape of the
clusters.
This can lead to counter-intuitive results, e.g., scale-dependence.
A method that also estimates the shape of clusters must take off-diagonal
entries of the scatter matrix into account.

\subsubsection{Silhouettes}

\begin{dfn}
  [Silhouette]
  Let:
  \begin{itemize}
    \item $X = \bigcup_{i = 1}^{k} X_i \in \mathbb{X}$ be a set of instances
          partitioned into $k$ clusters;
    \item $a(\vec{x}_j)$ be the average distance to the other instances in $X_i$:
          \begin{equation}
            a(\vec{x}_j) =
            \frac{1}{\card{X_i} - 1}
            \sum_{\vec{x} \in X_i, \vec{x} \neq \vec{x}_j} D(\vec{x}_j, \vec{x})
            \quad\forall\quad
            \vec{x}_j \in X_i,\ X_i \in \mathbb{X}
          \end{equation}
    \item $b(\vec{x}_j)$ be the average distance to the instances in the
          neighbouring cluster, i.e., the cluster with the nearest centroid:
          \begin{equation}
            b(\vec{x}_j) =
            \min_{X_i \in \mathbb{X}, X_i \neq X_j}
            \frac{1}{\card{X_j}}
            \sum_{\vec{x} \in X_j} D(\vec{x}_j, \vec{x})
            \quad\forall\quad
            \vec{x}_j \in X_i,\ X_i \in \mathbb{X}
          \end{equation}
  \end{itemize}
  The silhouette of $\vec{x}_j$ is:
  \begin{equation}
    s(\vec{x_j}) =
    \frac{b(\vec{x}_j) - a(\vec{x}_j)}{\max(a(\vec{x}_j), b(\vec{x}_j))}
  \end{equation}
  The silhouette of $X$ is a plot of $s(\vec{x}_j)$ against
  $\vec{x}_j$, grouped by $X_i$ and sorted in descending order of
  $s(\vec{x}_j)$.
\end{dfn}

\subsection{Hierarchical clustering}

\subsubsection{Dendrograms}

A \textit{dendrogram} is a tree diagram defined in terms of a distance metric.
It is a descriptive rather than a predictive model, because it partitions the
training set but not the entire instance space, and has high variance.
It requires a definition of the distance between clusters.

\begin{dfn}
  [Dendrogram]
  Let $X$ be a set of instances.
  A dendrogram is a binary tree where each leaf is an instance in $X$, each
  branch is a subset of instances, and the level of each node is the distance
  between the clusters represented by its children.
\end{dfn}

\subsubsection{Linkage functions}

A \textit{linkage function} translates pairwise distances between instances into
pairwise distances between clusters.

\begin{dfn}
  [Linkage function]
  Let $X$ be a set of instances and $D : X \times X \rightarrow \mathbb{R}$ be
  a distance metric.
  A linkage function $L : X \times X \rightarrow \mathbb{R}$ is the distance
  between $X_i, X_j \subseteq X$.
\end{dfn}

\begin{dfn}
  [Single linkage]
  The single linkage between $X_i, X_j \subseteq X$ is the minimum distance
  between any two instances in $X_i$ and $X_j$:
  \begin{equation}
    L(X_i, X_j) =
    \min_{\vec{x} \in X_i, \vec{y} \in X_j} D(\vec{x}, \vec{y})
  \end{equation}
\end{dfn}

\begin{dfn}
  [Complete linkage]
  The complete linkage between $X_i, X_j \subseteq X$ is the maximum distance
  between any two instances in $X_i$ and $X_j$:
  \begin{equation}
    L(X_i, X_j) =
    \max_{\vec{x} \in X_i, \vec{y} \in X_j} D(\vec{x}, \vec{y})
  \end{equation}
\end{dfn}

\begin{dfn}
  [Average linkage]
  The average linkage between $X_i, X_j \subseteq X$ is the average distance
  between any two instances in $X_i$ and $X_j$:
  \begin{equation}
    L(X_i, X_j) =
    \frac{1}{\card{X_i} \card{X_j}}
    \sum_{\vec{x} \in X_i, \vec{y} \in X_j} D(\vec{x}, \vec{y})
  \end{equation}
\end{dfn}

\begin{dfn}
  [Centroid linkage]
  The centroid linkage between $X_i, X_j \subseteq X$ is the distance between
  the centroids of $X_i$ and $X_j$:
  \begin{equation}
    L(X_i, X_j) = D\left(
    \frac{1}{\card{X_i}} \sum_{\vec{x} \in X_i} \vec{x},\
    \frac{1}{\card{X_j}} \sum_{\vec{y} \in X_j} \vec{y}
    \right)
  \end{equation}
\end{dfn}

\subsubsection{Hierarchical agglomerative clustering}

The algorithm to build a dendrogram is \textit{agglomerative} (it works from the
bottom up).
Generally, it produces different results for different linkage functions.
For single linkage, it adds links until there is a path between any two
instances.

Hierarchical clustering for single linkage effectively computes and sorts the
pairwise distances between instances, which takes $O(n^2)$ time.
For other linkage functions, it takes at least $O(n^2 \log n)$ time.
It is deterministic and always produces a clustering, which may not be
high-quality.
While the number of clusters does not need to be chosen in advance, both a
distance metric and a linkage function must be chosen.

Single and complete linkage do not take the shape of clusters into account
because they are defined in terms of the distance between a pair of instances.
However, centroid linkage can produce counter-intuitive results because it
violates \textit{monotonicity}.