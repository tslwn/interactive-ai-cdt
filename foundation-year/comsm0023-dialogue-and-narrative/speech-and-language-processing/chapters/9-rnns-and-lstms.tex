\newcommand{\suppar}[2]{#1^{\,(#2)}}

\section{RNNs and LSTMs}
\label{sec:9:rnn-lstm}

Language is inherently temporal.
Some language-processing algorithms are temporal, e.g., the Viterbi algorithm
(section~\ref{sec:8:viterbi-algorithm}), but others access the entire input
sequence at once, e.g., bag-of-words models.
Recurrent neural networks, e.g., LSTMs, represent prior context differently
to $n$-gram models or feedforward neural networks: they do not require a fixed
window size.

\subsection{Recurrent Neural Networks}
\label{sec:9:rnn}

A \textit{recurrent} neural network (RNN) has a cycle in its connections.
Generally, RNNs are difficult to train but constrained architectures like
\textit{Elman} or \textit{simple} recurrent networks have proved successful for
language modelling tasks.

\subsubsection{Inference}
\label{sec:9:rnn-inference}

For a simple RNN with one hidden layer, inference at time $t$ is computed by:
\begin{align}
  \suppar{\vec{h}}{t} & = f(\matr{W} \suppar{\vec{x}}{t} + \matr{U} \suppar{\vec{h}}{t - 1}) \\
  \suppar{\vec{y}}{t} & = g(\matr{V} \suppar{\vec{h}}{t})
\end{align}
\begin{itemize}
  \item $\suppar{\vec{x}}{t} \in \mathbb{R}^{n_x}$ is the input vector at time $t$;
  \item $\suppar{\vec{h}}{t} \in \mathbb{R}^{n_h}$ is the hidden-layer activation vector at time $t$;
  \item $\suppar{\vec{y}}{t} \in \mathbb{R}^{n_y}$ is the output vector at time $t$;
  \item $\matr{W} \in \mathbb{R}^{n_h \times n_x}$ is the weight matrix between the input and hidden layers;
  \item $\matr{U} \in \mathbb{R}^{n_h \times n_h}$ is the weight matrix between the hidden layer and itself;
  \item $\matr{V} \in \mathbb{R}^{n_y \times n_h}$ is the weight matrix between the hidden and output layers;
  \item $f : \mathbb{R}^{n_h} \to \mathbb{R}^{n_h}$ is the hidden-layer activation function; and
  \item $g : \mathbb{R}^{n_y} \to \mathbb{R}^{n_y}$ is the output-layer activation function.
\end{itemize}
The computation is incremental because the hidden-layer activation vector at
time $t$ depends on the hidden-layer activation vector at time $t - 1$.
The network is effectively \textit{unrolled} over time, i.e., the vectors are
copied for each time step, but the weight matrices are shared across time steps.

\subsubsection{Training}

Historically, RNNs were trained with \textit{backpropagation through time}, a
two-pass algorithm.
But generally, a feedforward network is generated from the structure of an RNN
for a given input sequence.
Long input sequences can be divided into fixed-length subsequences.

\subsection{Language modelling}
\label{sec:9:rnn-lm}

\subsubsection{Inference}
\label{sec:9:rnn-lm-inference}

For a simple recurrent language model with one hidden layer, inference at time
$t$ is computed by (section~\ref{sec:9:rnn-inference}):
\begin{align}
  \suppar{\vec{e}}{t} & = \matr{E} \suppar{\vec{x}}{t}                                       \\
  \suppar{\vec{h}}{t} & = f(\matr{W} \suppar{\vec{e}}{t} + \matr{U} \suppar{\vec{h}}{t - 1}) \\
  \suppar{\vec{y}}{t} & = \softmax(\matr{V} \suppar{\vec{h}}{t})
\end{align}
\begin{itemize}
  \item $\matr{X} = \begin{bmatrix} \vec{x}_1 & \dots & \vec{x}_n \end{bmatrix}
          \in \mathbb{R}^{n_x \times n}$ is the input matrix, i.e., a sequence
        of $n$ word embeddings represented by one-hot vectors of size $n_x$ (the
        vocabulary size);
  \item $\suppar{\vec{e}}{t} \in \mathbb{R}^{n_h}$ is the input embedding vector at time $t$; and
  \item $\matr{E} \in \mathbb{R}^{n_h \times n_x}$ is the input embedding matrix.
\end{itemize}

\subsubsection{Predicted probabilities}

The output vector $\suppar{\vec{y}}{t}$ is the predicted probability
distribution over the vocabulary.
The predicted probability that the next word is the $i$-th word is
$\suppar{y_i}{t}$, i.e., the $i$-th component of $\suppar{\vec{y}}{t}$:
\begin{equation}
  P(w_{t + 1} = i \mid w_{1} \dots w_t) = \suppar{y_i}{t}
\end{equation}
The predicted probability of a sequence of length $n$ is the product of the
probabilities of the words in the sequence:
\begin{equation}
  P(w_{1} \dots w_{n}) = \prod_{i = 1}^n \suppar{y_{w_i}}{i}
\end{equation}

\subsubsection{Training}
\label{sec:9:rnn-lm-training}

The model is trained to minimise the difference between the predicted and
correct probability distributions, which is measured by the cross-entropy loss
function.
The correct probability distribution is represented by a one-hot vector that is
1 for the next true word and 0 otherwise.
Therefore, the cross-entropy loss is solely determined by the predicted
probability of the next true word:
\begin{equation}
  \mathcal{L}(\suppar{\vec{\hat{y}}}{t}, \suppar{\vec{y}}{t})
  = -\log\suppar{\hat{y}_{w_{t + 1}}}{t}
\end{equation}

At each time $t$ of the input sequence, inference is performed with the true
sequence $w_1 \dots w_t$ to generate the predicted probability distribution
$\suppar{\vec{\hat{y}}}{t}$ and the cross-entropy loss is computed from the
predicted probability of the next true word $\suppar{\hat{y}_{w_{t + 1}}}{t}$.
The procedure is repeated at time $t + 1$, ignoring the predicted word at time
$t$.

\subsubsection{Weight tying}

The output vector $\suppar{\vec{y}}{t} \in \mathbb{R}^{n_x}$ of a language model
has the same dimensions as the input vector.
Therefore, the input embedding matrix $\matr{E} \in \mathbb{R}^{n_h \times n_x}$
and the weight matrix between the hidden and output layers
$\matr{V} \in \mathbb{R}^{n_x \times n_h}$ are similar: both can be interpreted
as word embeddings.

\textit{Weight tying} is the practice of setting $\matr{V} = \matr{E}^T$
(section~\ref{sec:9:rnn-lm-inference}), which reduces the number of parameters
of the model and improves its perplexity:
\begin{align}
  \suppar{\vec{e}}{t} & = \matr{E} \suppar{\vec{x}}{t}                                       \\
  \suppar{\vec{h}}{t} & = f(\matr{W} \suppar{\vec{e}}{t} + \matr{U} \suppar{\vec{h}}{t - 1}) \\
  \suppar{\vec{y}}{t} & = \softmax(\matr{E}^T \suppar{\vec{h}}{t})
\end{align}

\subsection{Other tasks}
\label{sec:9:rnn-other-tasks}

\subsubsection{Sequence labelling}

In an RNN for sequence labelling (section~\ref{sec:8:sequence-labelling}), the
input vectors are pre-trained embeddings and the output vectors are probability
distributions over the labels.
The RNN infers the most likely label at each time step and is trained as in
section~\ref{sec:9:rnn-lm-training}.

\subsubsection{Sequence classification}
\label{sec:9:rnn-sequence-classification}

In an RNN for sequence classification, the hidden layer for the last time step
is treated as a compressed representation of the entire sequence.
This representation can be input to a feedforward network for classification
(section~\ref{sec:7:nn-classification}).

Alternatively, a \textit{pooling} function can be used to represent the entire
sequence by an aggregate of the hidden layers at each time step, e.g., the
element-wise mean or maximum.
In this instance, the RNN is trained \textit{end-to-end}: the loss function is
based only on the classification task.

\subsubsection{Text generation}

The approach of incrementally generating text from a language model by
repeatedly sampling the next word from the predicted probability distribution
conditioned on the previous samples is called \textit{autoregressive} or causal
language-model generation.
Task-appropriate context can be provided by conditioning the model on a prefix
as well as the previous samples.

\subsection{Stacked RNNs}

In sections~\ref{sec:9:rnn} to \ref{sec:9:rnn-other-tasks}, the input vectors
are embeddings and the output vectors are probability distributions that predict
words or labels.
\textit{Stacked} RNNs consist of multiple networks, where the output vectors of
one network are the input vectors of the next.

Generally, stacked RNNs outperform single-layer networks.
They seem to induce representations at different levels of abstraction in the
different layers.
However, they are more expensive to train.

\subsection{Bidirectional RNNs}

In sections~\ref{sec:9:rnn} to \ref{sec:9:rnn-other-tasks}, the networks use
information from only the prior context (previous time steps).
However, for some tasks, the entire sequence can be accessed at once, i.e.,
information from subsequent time steps could also be used.
\textit{Bidirectional} RNNs consist of two networks, one that processes the
sequence from left to right and one that processes it from right to left.

The hidden-layer activation vectors of the two networks can be concatenated at
each time step to produce a single vector that represents both contexts:
\begin{equation}
  \suppar{\vec{h}}{t} = \begin{bmatrix}
    \suppar{\vec{h}}{t}_\text{left} & \suppar{\vec{h}}{t}_\text{right}
  \end{bmatrix}
\end{equation}
Alternatively, the two vectors can be combined with a pooling function, e.g.,
element-wise addition or multiplication.
Bidirectional RNNs have been successfully applied to sequence classification
(section~\ref{sec:9:rnn-sequence-classification}).

\subsection{Encoder-decoder models}

An encoder-decoder or \textit{sequence-to-sequence} model translates an input
sequence into an output of a different length.
This model architecture is commonly applied to machine translation,
summarisation, question answering, dialogue, etc.

The architecture has three components:
\begin{itemize}
  \item an \textit{encoder} takes an input sequence $\vec{x}_0 \cdots \vec{x}_n$
        and generates a sequence of representations
        $\vec{h}^{(e)}_0 \cdots \vec{h}^{(e)}_n$;
  \item the sequence of representations generates a \textit{context vector}
        $\vec{c}$; and
  \item a \textit{decoder} takes a context vector $\vec{c}$ and generates a
        sequence of hidden states $\vec{h}^{(d)}_0 \cdots \vec{h}^{(d)}_n$ and
        an output sequence $\vec{y}_0 \cdots \vec{y}_n$.
\end{itemize}
The encoder and decoder variants of a recurrent network.
The equations to compute the output $y_t$ at time $t$ are:
\begin{multline}
  \vec{c}
  = \vec{h}^{(e)}_n
  ,\quad
  \vec{h}^{(d)}_{0}
  = \vec{c}
  ,\quad
  \vec{h}^{(d)}_t
  = \text{RNN}(\hat{y}_{t - 1},\, \vec{h}^{(d)}_{t - 1},\, \vec{c})
  \\
  \vec{z}_t
  = f(\vec{h}^{(d)}_t)
  ,\quad
  \vec{y}_t
  = \softmax(\vec{z}_t)
  ,\quad
  \hat{y}_t
  = \argmax_{w \in \mathcal{V}} \vec{y}_t(w)
\end{multline}

\paragraph{Training}

Encoder-decoder models are trained end-to-end with pairs of input and output
texts, e.g., aligned sentence pairs for machine translation.
For inference, the decoder uses its estimated output $\hat{y}_t$ as the input to
the next time step $x_{t + 1}$, which leads the decoder to deviate from the true
output.
For training, it is common to use \textit{teacher forcing}, i.e., for the
decoder to use the true output $y_t$ as the input to the next time step instead.

\subsection{Attention}

In a simple encoder-decoder model, the context vector $\vec{c}$ is the last
hidden state of the encoder.
The last hidden state may act as a \textit{bottleneck}, i.e., it may not be a
good representation of the entire input text.
The \textit{attention} mechanism is a way to allow the decoder to access
information from all the hidden states of the encoder.

The context vector becomes a function of the encoder's hidden states:
\begin{equation}
  \vec{c} = f(\vec{h}^{(e)}_1, \dots, \vec{h}^{(e)}_n)
\end{equation}
The weightings that the function applies to the hidden states `pay attention to'
the parts of the input sequence that are relevant to the output at a given time
step.
The decoder's hidden state is conditioned on the context vector:
\begin{equation}
  \vec{h}^{(d)}_i = \text{RNN}(\hat{y}_{i - 1},\, \vec{h}^{(d)}_{i - 1},\, \vec{c}_{i - 1})
\end{equation}

The relevance of the encoder states to the decoder state $\vec{h}^{(d)}_i$ is
computed via a score $(\vec{h}^{(d)}_{i - 1}, \vec{h}^{(e)}_j)$.
The simplest example of a scoring function is \textit{dot-product} attention:
$\vec{h}^{(d)}_{i - 1} \cdot \vec{h}^{(e)}_j$.
The scores are normalised with softmax to obtain a vector of weights:
\begin{equation}
  \alpha^{(i)}_j
  = \softmax(\vec{h}^{(d)}_{i - 1} \cdot \vec{h}^{(e)}_j)
  = \frac{\exp(\vec{h}^{(d)}_{i - 1} \cdot \vec{h}^{(e)}_j)}{\sum_{k = 1}^n \exp(\vec{h}^{(d)}_{i - 1} \cdot \vec{h}^{(e)}_k)}
\end{equation}
Finally, given $\alpha^{(i)}_1, \dots, \alpha^{(i)}_n$, the context vector is:
\begin{equation}
  \vec{c}_i = \sum_{j = 1}^n \alpha^{(i)}_j \vec{h}^{(e)}_j
\end{equation}

A more complex scoring function has its own weights $\matr{W}_{s}$:
\begin{equation}
  \alpha^{(i)}_j = \softmax(\vec{h}^{(d)}_{i - 1} \matr{W}_{s} \vec{h}^{(e)}_j)
\end{equation}
The weights are learned in end-to-end training and allow the network to learn
which aspects of similarity between the encoder and decoder hidden states are
relevant to the task.
They also allow the hidden states to have different dimensionality, unlike the
dot-product scoring function.