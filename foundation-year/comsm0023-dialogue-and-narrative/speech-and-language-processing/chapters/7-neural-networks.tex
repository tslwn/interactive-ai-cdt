\section{Neural Networks}

\subsection{Units}

\begin{dfn}
  [Computational unit]
  \label{dfn:7:ComputationalUnit}
  A computational unit is a function that takes an input vector $\vec{x}$, a
  weight vector $\vec{x}$, a scalar bias $b$, and a non-linear function $f$ and
  produces an activation $y$:
  \begin{equation}
    y = f(\vec{x} \cdot \vec{x} + b)
  \end{equation}
\end{dfn}

\paragraph{Non-linear functions}

Examples of non-linear functions are:
\begin{itemize}
  \item the sigmoid $\sigma(z) = \frac{1}{1 + e^{-z}}$;
  \item the hyperbolic tangent $\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$; and
  \item the rectified linear unit (ReLU) $\max(0, z)$.
\end{itemize}

$\tanh$ is differentiable and maps outliers towards the mean, whereas ReLU is
non-differentiable.
For large $z$ values, $\sigma$ and $\tanh$ produce saturated $y$ values, i.e.,
values that are nearly one have gradients close to zero, whereas ReLU has
gradient one for positive $z$ values.
Gradients that are close to zero cannot be used for training (the
\textit{vanishing gradient} problem).

\subsection{The XOR problem}

A single computational unit (e.g., a \textit{perceptron}) cannot compute simple
functions of its inputs (e.g., XOR).
This is because a perceptron is a linear classifier and XOR is not a linearly
separable function.
However, a layered network of units can compute functions like XOR.

\subsection{Feedforward neural networks}

A \textit{feedforward} network is a layered network of units that are connected
with no cycles.
Networks with cycles are called \textit{recurrent} networks.
Feedforward networks are sometimes called \textit{multi-layer perceptrons} but
this is a misnomer unless the units are perceptrons.

A simple feedforward network has input units, hidden units, and output units.
In a \textit{fully-connected} layer of units, each unit takes as its input the
outputs of all units in the previous layer.

\paragraph{Hidden layers}

A hidden layer is a set of computational units
(definition~\ref{dfn:7:ComputationalUnit}) that takes an input vector
$\vec{x}$, a weight matrix $\matr{W}$, a bias vector $\vec{b}$, and a
non-linear function $f$ and produces an activation vector $\vec{h}$:
\begin{equation}
  \vec{h} = f(\matr{W} \vec{x} + \vec{b})
\end{equation}
The function $f$ is applied element-wise to the vector
$\matr{W} \vec{x} + \vec{b}$.

\paragraph{Output layers}

In a feedforward network with a single hidden layer, the output layer is a
function that takes an activation vector $\vec{h}$ and a weight matrix
$\matr{U}$ and produces an intermediate output vector $\vec{z}$:
\begin{equation}
  \vec{z} = \matr{U} \vec{h}
\end{equation}

\paragraph{Normalization functions}

A \textit{normalization} function converts an activation vector into a vector
that represents a probability distribution, e.g., softmax:
\begin{equation}
  \vec{y} = \softmax(\vec{z}) = \frac{e^{\vec{z}}}{\sum_{i = 1}^{n} e^{z_i}}
\end{equation}

\paragraph{Dimensionality}

The elements of feedforward neural network with a single fully-connected hidden
layer have the following dimensions:
\begin{itemize}
  \item $\vec{x} \in \mathbb{R}^{n_0}$ is a column vector $n_0 \times 1$;
  \item $\vec{h}, \vec{b}, \vec{z} \in \mathbb{R}^{n_1}$ are column vectors $n_1 \times 1$;
  \item $\matr{W} \in \mathbb{R}^{n_1 \times n_0}$ is a matrix $n_1 \times n_0$;
  \item $\matr{U} \in \mathbb{R}^{n_2 \times n_1}$ is a matrix $n_2 \times n_1$; and
  \item $\vec{y} \in \mathbb{R}^{n_2}$ is a column vector $n_2 \times 1$.
\end{itemize}

\paragraph{Non-linear activation functions}

If the activation function $f$ is linear, then a multi-layer feedforward network
is equivalent to a single-layer network (with a different weight matrix).
Therefore, we use non-linear activation functions.

\paragraph{Replacing the bias vector}

Generally, we replace the bias vector with an additional unit in each layer
whose activation is always one.
The weights associated with this unit are the bias value $b$:
\begin{align*}
  \vec{x} \in \mathbb{R}^{n_0}
   & \rightarrow \vec{x} \in \mathbb{R}^{n_0 + 1},\
  x_0 = 1                                                         \\
  \matr{W} \in \mathbb{R}^{n_1 \times n_0}
   & \rightarrow \matr{W} \in \mathbb{R}^{n_1 \times (n_0 + 1)},\
  W_{i0} = b_i \ \forall\ i \in 0 .. n_0                          \\
\end{align*}

\subsection{Classification}

A classifier could use hand-crafted features but most applications learn
features from the data by representing words by embeddings.
An input is generally represented by applying a \textit{pooling} function to the
embeddings of its words, e.g., the mean or element-wise maximum.

For a two-layer classifier, the equations to predict the output of a set of test
instances are:
\begin{itemize}
  \item $n_i$ is the number of test instances;
  \item $n_j$ is the dimensionality of the instance space;
  \item $n_k$ is the number of nodes in the hidden layer;
  \item $n_l$ is the number of classes;
  \item $\matr{X} \in \mathbb{R}^{n_i \times n_j}$ is the set of test instances;
  \item $\matr{W} \in \mathbb{R}^{n_k \times n_j}$ is the weight matrix;
  \item $\matr{B} \in \mathbb{R}^{n_i \times n_k}$ is the bias matrix;
  \item $\matr{H} = \sigma(\matr{X} \matr{W}^{T} + \matr{B}) \in \mathbb{R}^{n_i \times n_k}$
        is the hidden-layer activation matrix;
  \item $\matr{U} \in \mathbb{R}^{n_l \times n_k}$ is the hidden-layer weight matrix;
  \item $\matr{Z} = \matr{H} \matr{U}^{T} \in \mathbb{R}^{n_i \times n_l}$ is the intermediate output matrix; and
  \item $\matr{\hat{Y}} = \softmax(\matr{Z}) \in \mathbb{R}^{n_i \times n_l}$ is the output matrix.
\end{itemize}

The use of an input representation is called \textit{pretraining}.
It is possible to train the classifier and the input representation jointly as
part of an NLP task.

\subsection{Language modelling}

The task of \textit{language modelling} is to predict upcoming words from prior
context.
Neural language modelling is an important task in itself and a precursor to many
others.
Generally, modern language models use architectures like recurrent neural
networks or transformers.

\subsubsection{Inference}

Forward inference is the task of producing a probability distribution over the
possible outputs, given an input.
For language models, the inputs and outputs are words.
A \textit{one-hot vector} has a single element equal to 1 and the rest 0.

The equations to predict the output are:
\begin{itemize}
  \item $n_i$ is the context window size;
  \item $n_j$ is the number of words in the vocabulary;
  \item $n_k$ is the dimensionality of the embeddings;
  \item $n_l$ is the number of nodes in the hidden layer;
  \item $\{ x_{t - i} \mid i = 1 .. n_i \} \in \mathbb{R}^{n_j}$ are one-hot word vectors;
  \item $\matr{E} \in \mathbb{R}^{n_k \times n_j}$ is the embedding matrix;
  \item $\vec{e}_i = \matr{E} \vec{x}_i \in \mathbb{R}^{n_k}$ is the embedding
        of word $x_i$;
  \item $\vec{e} = [\vec{e}_1; \ldots; \vec{e}_{n_i}] \in \mathbb{R}^{n_i n_k}$
        are the concatenated embeddings;
  \item $\matr{W} \in \mathbb{R}^{n_l \times n_i n_k}$ is the weight matrix;
  \item $\vec{b} \in \mathbb{R}^{n_l}$ is the bias vector;
  \item $\vec{h} = \sigma(\matr{W} \vec{e} + \vec{b}) \in \mathbb{R}^{n_l}$ is
        the hidden-layer activation vector;
  \item $\matr{U} \in \mathbb{R}^{n_j \times n_l}$ is the hidden-layer weight matrix;
  \item $\vec{z} = \matr{U} \vec{h} \in \mathbb{R}^{n_j}$ is the intermediate
        output vector; and
  \item $\vec{\hat{y}} = \softmax(\vec{z}) \in \mathbb{R}^{n_j}$ is the output
        vector.
\end{itemize}

\subsection{Training neural networks}

A feedforward neural network is an example of a supervised machine learning model.
A neural network is trained with:
\begin{itemize}
  \item a loss function, e.g., the cross-entropy loss
        (section~\ref{sec:7:cross-entropy-loss}); and
  \item an optimisation algorithm, e.g., gradient descent.
\end{itemize}
For logistic regression, the derivative of the loss function can be computed
directly (section~\ref{sec:7:gradient}).
For neural networks, \textit{backpropagation}
(section~\ref{sec:7:backward-differentiation}) is necessary.

\subsubsection{Cross-entropy loss}
\label{sec:7:cross-entropy-loss}

\begin{dfn}
  [Cross-entropy loss]
  Let $\vec{y}, \vec{\hat{y}} \in \mathbb{R}^k$ be the one-hot true output
  vector and predicted output vector, respectively.
  The cross-entropy loss of an instance with true class $j$ is:
  \begin{align}
    L(\vec{y}, \vec{\hat{y}})
     & = -\sum_{i = 1}^{k} y_i \log \hat{y}_i \\
     & = -\log \hat{y}_j
  \end{align}
\end{dfn}

\subsubsection{Gradient}
\label{sec:7:gradient}

For a neural network with a single hidden layer and sigmoid activation function,
i.e., logistic regression, the derivative of the cross-entropy loss with respect
to the weight $w_i$ is:
\begin{align*}
  \frac{\partial L(\vec{y}, \vec{\hat{y}})}{\partial w_i}
   & = (\hat{y} - y) \vec{x}_i                           \\
   & = (\sigma(\vec{w} \cdot \vec{x} + b) - y) \vec{x}_i
\end{align*}

With a softmax activation function, i.e., multinomial logistic regression, the
derivative of the cross-entropy loss with respect to the weight $w_{ij}$ is:
\begin{align*}
  \frac{\partial L(\vec{y}, \vec{\hat{y}})}{\partial w_{ij}}
   & = - (\vec{y}_i - \vec{\hat{y}}_i) \vec{x}_j \\
   & = - \left(
  \vec{y}_i -
  \frac{ \exp(\vec{w}_i \cdot \vec{x} + b_i)}{\sum_{k = 1}^{l} \exp(\vec{w}_k \cdot \vec{x} + b_k)}
  \right) \vec{x}_j
\end{align*}

\subsubsection{Backward differentiation}
\label{sec:7:backward-differentiation}

For a neural network with multiple hidden layers, we must compute the derivative
with respect to the weights in each layer.
This is achieved by backpropagation or backprop, which is a form of
\textit{backward differentiation} on the computational graph of the network.

\paragraph{Example} TODO

\subsubsection{Learning}

For logistic regression, we can initialize the weights and biases to zero.
For neural networks, we must initialize the weights and biases to small random
numbers and normalize the inputs (zero mean and unit variance).

Regularization, e.g., \textit{dropout}, is used to prevent overfitting.
It is also important to tune \textit{hyperparameters}, e.g., the learning rate,
the mini-batch size, the model architecture, and the regularization.


