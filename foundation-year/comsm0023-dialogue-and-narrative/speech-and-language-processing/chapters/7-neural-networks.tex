\section{Neural Networks}

\subsection{Units}

\begin{dfn}
  [Computational unit]
  \label{dfn:7:ComputationalUnit}
  A computational unit is a function that takes an input vector $\vec{x}$, a
  weight vector $\vec{x}$, a scalar bias $b$, and a non-linear function $f$ and
  produces an activation $y$:
  \begin{equation}
    y = f(\vec{x} \cdot \vec{x} + b)
  \end{equation}
\end{dfn}

\paragraph{Non-linear functions}

Examples of non-linear functions are:
\begin{itemize}
  \item the sigmoid $\sigma(z) = \frac{1}{1 + e^{-z}}$;
  \item the hyperbolic tangent $\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$; and
  \item the rectified linear unit (ReLU) $\max(0, z)$.
\end{itemize}

$\tanh$ is differentiable and maps outliers towards the mean, whereas ReLU is
non-differentiable.
For large $z$ values, $\sigma$ and $\tanh$ produce saturated $y$ values, i.e.,
values that are nearly one have gradients close to zero, whereas ReLU has
gradient one for positive $z$ values.
Gradients that are close to zero cannot be used for training (the
\textit{vanishing gradient} problem).

\subsection{The XOR problem}

A single computational unit (e.g., a \textit{perceptron}) cannot compute simple
functions of its inputs (e.g., XOR).
This is because a perceptron is a linear classifier and XOR is not a linearly
separable function.
However, a layered network of units can compute functions like XOR.

\subsection{Feedforward neural networks}

\paragraph{Feedforward networks}

A \textit{feedforward} network is a layered network of units that are connected
with no cycles.
Networks with cycles are called \textit{recurrent} networks.
Feedforward networks are sometimes called \textit{multi-layer perceptrons} but
this is a misnomer unless the units are perceptrons.

A simple feedforward network has input units, hidden units, and output units.
In a \textit{fully-connected} layer of units, each unit takes as its input the
outputs of all units in the previous layer.

\paragraph{Hidden layers}

A hidden layer is a set of computational units
(definition~\ref{dfn:7:ComputationalUnit}) that takes an input vector
$\vec{x}$, a weight matrix $\matr{W}$, a bias vector $\vec{b}$, and a
non-linear function $f$ and produces an activation vector $\vec{h}$:
\begin{equation}
  \vec{h} = f(\matr{W} \vec{x} + \vec{b})
\end{equation}
The function $f$ is applied element-wise to the vector
$\matr{W} \vec{x} + \vec{b}$.

\paragraph{Output layers}

In a feedforward network with a single hidden layer, the output layer is a
function that takes an activation vector $\vec{h}$ and a weight matrix
$\matr{U}$ and produces an intermediate output vector $\vec{z}$:
\begin{equation}
  \vec{z} = \matr{U} \vec{h}
\end{equation}

\paragraph{Normalization functions}

A \textit{normalization} function converts an activation vector into a vector
that represents a probability distribution, e.g., softmax:
\begin{equation}
  \vec{y} = \softmax(\vec{z}) = \frac{e^{\vec{z}}}{\sum_{i = 1}^{n} e^{z_i}}
\end{equation}

\paragraph{Dimensionality}

The elements of feedforward neural network with a single fully-connected hidden
layer have the following dimensions:
\begin{itemize}
  \item $\vec{x} \in \mathbb{R}^{n_0}$ is a column vector $n_0 \times 1$;
  \item $\vec{h}, \vec{b}, \vec{z} \in \mathbb{R}^{n_1}$ are column vectors $n_1 \times 1$;
  \item $\matr{W} \in \mathbb{R}^{n_1 \times n_0}$ is a matrix $n_1 \times n_0$;
  \item $\matr{U} \in \mathbb{R}^{n_2 \times n_1}$ is a matrix $n_2 \times n_1$; and
  \item $\vec{y} \in \mathbb{R}^{n_2}$ is a column vector $n_2 \times 1$.
\end{itemize}

\paragraph{Non-linear activation functions}

If the activation function $f$ is linear, then a multi-layer feedforward network
is equivalent to a single-layer network (with a different weight matrix).
Therefore, we use non-linear activation functions.

\paragraph{Replacing the bias vector}

Generally, we replace the bias vector with an additional unit in each layer
whose activation is always one.
The weights associated with this unit are the bias value $b$:
\begin{align*}
  \vec{x} \in \mathbb{R}^{n_0}
   & \rightarrow \vec{x} \in \mathbb{R}^{n_0 + 1},\
  x_0 = 1                                                         \\
  \matr{W} \in \mathbb{R}^{n_1 \times n_0}
   & \rightarrow \matr{W} \in \mathbb{R}^{n_1 \times (n_0 + 1)},\
  W_{i0} = b_i \ \forall\ i \in 0 .. n_0                          \\
\end{align*}

\subsection{Classification}

A classifier could use hand-crafted features but most applications learn
features from the data by representing words by embeddings.
An input is generally represented by applying a \textit{pooling} function to the
embeddings of its words, e.g., the mean or element-wise maximum.

For a two-layer classifier, the equations to predict the output of a set of test
instances are:
\begin{itemize}
  \item $n_i$ is the number of test instances;
  \item $n_j$ is the dimensionality of the instance space;
  \item $n_k$ is the number of nodes in the hidden layer;
  \item $n_l$ is the number of classes;
  \item $\matr{X} \in \mathbb{R}^{n_i \times n_j}$ is the set of test instances;
  \item $\matr{W} \in \mathbb{R}^{n_k \times n_j}$ is the weight matrix;
  \item $\matr{B} \in \mathbb{R}^{n_i \times n_k}$ is the bias matrix;
  \item $\matr{H} = \sigma(\matr{X} \matr{W}^{T} + \matr{B}) \in \mathbb{R}^{n_i \times n_k}$
        is the hidden-layer activation matrix;
  \item $\matr{U} \in \mathbb{R}^{n_l \times n_k}$ is the hidden-layer weight matrix;
  \item $\matr{Z} = \matr{H} \matr{U}^{T} \in \mathbb{R}^{n_i \times n_l}$ is the intermediate output matrix; and
  \item $\matr{\hat{Y}} = \softmax(\matr{Z}) \in \mathbb{R}^{n_i \times n_l}$ is the output matrix.
\end{itemize}

The use of an input representation is called \textit{pretraining}.
It is possible to train the classifier and the input representation jointly as
part of an NLP task.

\subsection{Language modelling}

The task of \textit{language modelling} is to predict upcoming words from prior
context.
Neural language modelling is an important task in itself and a precursor to many
others.
Generally, modern language models use architectures like recurrent neural
networks or transformers.

\subsubsection{Inference}

Forward inference is the task of producing a probability distribution over the
possible outputs, given an input.
For language models, the inputs and outputs are words.
A \textit{one-hot vector} has a single element equal to 1 and the rest 0.

The equations to predict the output are:
\begin{itemize}
  \item $n_i$ is the context window size;
  \item $n_j$ is the number of words in the vocabulary;
  \item $n_k$ is the dimensionality of the embeddings;
  \item $n_l$ is the number of nodes in the hidden layer;
  \item $\{ x_{t - i} \mid i = 1 .. n_i \} \in \mathbb{R}^{n_j}$ are one-hot word vectors;
  \item $\matr{E} \in \mathbb{R}^{n_k \times n_j}$ is the embedding matrix;
  \item $\vec{e}_i = \matr{E} \vec{x}_i \in \mathbb{R}^{n_k}$ is the embedding
        of word $x_i$;
  \item $\vec{e} = [\vec{e}_1; \ldots; \vec{e}_{n_i}] \in \mathbb{R}^{3 n_k}$
        are the concatenated embeddings;
  \item $\matr{W} \in \mathbb{R}^{n_l \times 3 n_k}$ is the weight matrix;
  \item $\vec{b} \in \mathbb{R}^{n_l}$ is the bias vector;
  \item $\vec{h} = \sigma(\matr{W} \vec{e} + \vec{b}) \in \mathbb{R}^{n_l}$ is
        the hidden-layer activation vector;
  \item $\matr{U} \in \mathbb{R}^{n_j \times n_l}$ is the hidden-layer weight matrix;
  \item $\vec{z} = \matr{U} \vec{h} \in \mathbb{R}^{n_j}$ is the intermediate
        output vector; and
  \item $\vec{\hat{y}} = \softmax(\vec{z}) \in \mathbb{R}^{n_j}$ is the output
        vector.
\end{itemize}

\subsection{TODO}

\begin{itemize}
  \item Loss functions (cross-entropy)
  \item Gradients
  \item Backpropagation
\end{itemize}