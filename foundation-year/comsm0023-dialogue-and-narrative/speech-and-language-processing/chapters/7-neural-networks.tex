\section{Neural Networks}

\subsection{Units}

\begin{dfn}
  [Computational unit]
  \label{dfn:7:ComputationalUnit}
  A computational unit is a function that takes an input vector $\vec{x}$, a
  weight vector $\vec{w}$, a scalar bias $b$, and a non-linear function $f$ and
  produces an activation $y$:
  \begin{equation}
    y = f(\vec{x} \cdot \vec{w} + b)
  \end{equation}
\end{dfn}

\paragraph{Non-linear functions}

Examples of non-linear functions are:
\begin{itemize}
  \item the sigmoid $\sigma(z) = \frac{1}{1 + e^{-z}}$;
  \item the hyperbolic tangent $\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$; and
  \item the rectified linear unit (ReLU) $\max(0, z)$.
\end{itemize}

$\tanh$ is differentiable and maps outliers towards the mean, whereas ReLU is
non-differentiable.
For large $z$ values, $\sigma$ and $\tanh$ produce saturated $y$ values, i.e.,
values that are nearly one have gradients close to zero, whereas ReLU has
gradient one for positive $z$ values.
Gradients that are close to zero cannot be used for training (the
\textit{vanishing gradient} problem).

\subsection{The XOR problem}

A single computational unit (e.g., a \textit{perceptron}) cannot compute simple
functions of its inputs (e.g., XOR).
This is because a perceptron is a linear classifier and XOR is not a linearly
separable function.
However, a layered network of units can compute functions like XOR.

\subsection{Feedforward neural networks}

\paragraph{Feedforward networks}

A \textit{feedforward} network is a layered network of units that are connected
with no cycles.
Networks with cycles are called \textit{recurrent} networks.
Feedforward networks are sometimes called \textit{multi-layer perceptrons} but
this is a misnomer unless the units are perceptrons.

A simple feedforward network has input units, hidden units, and output units.
In a \textit{fully-connected} layer of units, each unit takes as its input the
outputs of all units in the previous layer.

\paragraph{Hidden layers}

A hidden layer is a set of computational units
(definition~\ref{dfn:7:ComputationalUnit}) that takes an input vector
$\vec{x}$, a weight matrix $\matr{W}$, a bias vector $\vec{b}$, and a
non-linear function $f$ and produces an activation vector $\vec{h}$:
\begin{equation}
  \vec{h} = f(\matr{W} \vec{x} + \vec{b})
\end{equation}
The function $f$ is applied element-wise to the vector
$\matr{W} \vec{x} + \vec{b}$.

\paragraph{Output layers}

In a feedforward network with a single hidden layer, the output layer is a
function that takes an activation vector $\vec{h}$ and a weight matrix
$\matr{U}$ and produces an intermediate output vector $\vec{z}$:
\begin{equation}
  \vec{z} = \matr{U} \vec{h}
\end{equation}

\paragraph{Normalization functions}

A \textit{normalization} function converts an activation vector into a vector
that represents a probability distribution, e.g., softmax:
\begin{equation}
  \vec{y} = \softmax(\vec{z}) = \frac{e^{\vec{z}}}{\sum_{i = 1}^{n} e^{z_i}}
\end{equation}

\paragraph{Dimensionality}

The elements of feedforward neural network with a single fully-connected hidden
layer have the following dimensions:
\begin{itemize}
  \item $\vec{x} \in \mathbb{R}^{n_0}$ is a column vector $n_0 \times 1$;
  \item $\vec{h}, \vec{b}, \vec{z} \in \mathbb{R}^{n_1}$ are column vectors $n_1 \times 1$;
  \item $\matr{W} \in \mathbb{R}^{n_1 \times n_0}$ is a matrix $n_1 \times n_0$;
  \item $\matr{U} \in \mathbb{R}^{n_2 \times n_1}$ is a matrix $n_2 \times n_1$; and
  \item $\vec{y} \in \mathbb{R}^{n_2}$ is a column vector $n_2 \times 1$.
\end{itemize}

\paragraph{Non-linear activation functions}

If the activation function $f$ is linear, then a multi-layer feedforward network
is equivalent to a single-layer network (with a different weight matrix).
Therefore, we use non-linear activation functions.

\paragraph{Replacing the bias vector}

Generally, we replace the bias vector with an additional unit in each layer
whose activation is always one.
The weights associated with this unit are the bias value $b$:
\begin{align*}
  \vec{x} \in \mathbb{R}^{n_0}
   & \rightarrow \vec{x} \in \mathbb{R}^{n_0 + 1},\
  x_0 = 1                                                         \\
  \matr{W} \in \mathbb{R}^{n_1 \times n_0}
   & \rightarrow \matr{W} \in \mathbb{R}^{n_1 \times (n_0 + 1)},\
  W_{i0} = b_i \ \forall\ i \in 0 .. n_0                          \\
\end{align*}

\subsection{TODO}

\begin{itemize}
  \item Example: classification
  \item Example: language modelling
  \item Loss functions (cross-entropy)
  \item Gradients
  \item Backpropagation
\end{itemize}