% !TEX root=week-4-vector-semantics.tex

\documentclass[10pt]{beamer}
\usepackage{../../../../latex/packages/tslwn-preamble}
\usepackage{../../../../latex/packages/tslwn-slides}

\title{Vector Semantics}
\author{Tim Lawson}

\begin{document}
\maketitle

\begin{frame}
  {The distributional hypothesis}
  \textquote{For a \textit{large} class of cases of the employment of the word
    ``meaning" — though not for \textit{all} — this word can be explained in
    this way: the meaning of a word is its use in the language.}
  \footcite[25]{Wittgenstein2010}
\end{frame}

\begin{frame}
  {The distributional hypothesis}
  \textquote{...we will often find interesting distributional relations,
    relations which tell us something about the occurrence of elements and which
    correlate with some aspect of meaning. In certain important cases it will
    even prove possible to state certain aspects of meaning as functions of
    measurable distributional relations.}
  \footcite[156]{Harris1954}
\end{frame}

\begin{frame}
  {The distributional hypothesis}
  \textquote{In these examples, the word \textit{ass} is in familiar and
    habitual company, commonly collocated with \textit{you silly—}, \textit{he
      is a silly—}, \textit{don't be such an—}. You shall know a word by the
    company it keeps!}
  \footcite[11]{Firth1957}
\end{frame}

\begin{frame}
  {Distributional (vector-space) semantics}
  \begin{itemize}
    \item What aspects of meaning can we talk about?
    \item What ``distributional relations" can we use?
    \item How do choices of relations affect the outcomes?
    \item What can we do with a vector? With multiple vectors?
  \end{itemize}
\end{frame}

\begin{frame}
  {Word embeddings}
  \begin{itemize}
    \item Term-document and term-term vectors
    \item[]
          \begin{itemize}
            \item Many-dimensional ($d = $ number of terms $\times$ number of
                  documents)
            \item Sparse (mostly zero)
          \end{itemize}
    \item Word embeddings
    \item[]
          \begin{itemize}
            \item Lower-dimensional ($d \approx 50-1000$)
            \item Dense (mostly non-zero)
          \end{itemize}
    \item ``Dense vectors work better in every NLP task''
          \footcite[119]{Jurafsky2023}
  \end{itemize}
\end{frame}

\begin{frame}
  {Skip-gram with negative sampling}
  \begin{itemize}
    \item Skip-gram: ``maximize[s] classification of a word based on another
          word in the same sentence" \footcite[4]{Mikolov2013}
    \item Negative sampling: ``distinguish[es] the target word $w_0$ from draws
          from the noise distribution $P_n(w)$ ... where there are $k$ negative
          samples for each data sample" \footcite[4]{Mikolov2013a}
  \end{itemize}
\end{frame}

\begin{frame}
  {Skip-gram with negative sampling}
  {Feature engineering}
  \begin{itemize}
    \item \href
          {https://github.com/tslwn/dialogue_and_narrative/blob/main/exercises/word2vec.py\#L19-L52}
          {Find unigram counts and weights}
    \item \href
          {https://github.com/tslwn/dialogue_and_narrative/blob/main/exercises/word2vec.py\#L97-L106}
          {Find $n$-grams where $n - 1$ is the window size}
    \item \href
          {https://github.com/tslwn/dialogue_and_narrative/blob/main/exercises/word2vec.py\#L79-L95}
          {Find positive examples (target-context word pairs)}
  \end{itemize}
\end{frame}

\begin{frame}
  {Skip-gram with negative sampling}
  {Training}
  \begin{itemize}
    \item \href
          {https://github.com/tslwn/dialogue_and_narrative/blob/main/exercises/word2vec.py\#L154-L156}
          {Initialize target- and context-word matrices}
    \item \href
          {https://github.com/tslwn/dialogue_and_narrative/blob/main/exercises/word2vec.py\#L180-L185}
          {Iterate until stopping criterion is met}
    \item[]
          \begin{itemize}
            \item \href
                  {https://github.com/tslwn/dialogue_and_narrative/blob/main/exercises/word2vec.py\#L202-L222}
                  {Update the positive context-word vector}
            \item \href
                  {https://github.com/tslwn/dialogue_and_narrative/blob/main/exercises/word2vec.py\#L54-L60}
                  {Find negative examples by sampling from the vocabulary}
            \item \href
                  {https://github.com/tslwn/dialogue_and_narrative/blob/main/exercises/word2vec.py\#L224-L256}
                  {Update the negative context- and target-word vectors}
          \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  {Semantic similarity}
  \begin{itemize}
    \item First-order co-occurrence (syntagmatic association)
    \item[]
          \begin{itemize}
            \item ``I \textit{wrote} a \textit{book}.''
          \end{itemize}
    \item Second-order co-occurrence (paradigmatic association)
    \item[]
          \begin{itemize}
            \item ``I \textit{wrote/read} a book.''
          \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  {Semantic similarity}
  \begin{itemize}
    \item Syno- and antonyms
    \item[]
          \begin{itemize}
            \item ``I took the \textit{bus/coach}.''
            \item ``The matrix is \textit{sparse/dense}.''
          \end{itemize}
    \item Hyper- and hyponyms
    \item[]
          \begin{itemize}
            \item ``I had a \textit{coffee/espresso}.''
          \end{itemize}
    \item Metonyms
    \item[]
          \begin{itemize}
            \item ``\textit{Number 10/The government} said that...''
          \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  {Visualizing embeddings}
  \begin{itemize}
    \item Nearest neighbours
    \item Clustering
    \item Dimensionality reduction
  \end{itemize}
\end{frame}

\begin{frame}
  {Bibliography}
  \printbibliography
\end{frame}

\end{document}