\subsection{Bayesian networks}

\subsubsection{Joint and marginal probability distributions}

\begin{dfn}[Joint probability distribution]
  The joint probability distribution of
  ${X = \{ X_i : W \to \Omega \mid i \in 1 .. n \}}$ is:
  \begin{align}
    P(X) & = P \bigl( \bigwedge_{i = 1}^{n} X_i = x_i \bigr) \nonumber
    \\
         & = P(\{
    w : X_1(w) = x_1, \ldots, X_n(w) = x_n \})
  \end{align}
\end{dfn}

If $|\{ X_i(w) : w \in W \}| = k_i$, then the joint
probability distribution has $k_i^n - 1$ values, i.e., the number of values
grows exponentially with the dimension of the random variables (the
\textit{curse of dimensionality}).

\begin{dfn}[Marginal probability distribution]
  The marginal probability distribution of $X_i : W \to \Omega$ is:
  \begin{equation}
    P(X_i = x_i) = P(\{ w : X_i(w) = x_i \})
    \ \forall\
    i \in 1 .. n
  \end{equation}
\end{dfn}

\begin{thm}[Joint and marginal probability distributions]
  The joint and marginal probability distributions are related by:
  \begin{equation}
    P(X_i = x_i) = \sum_{j \neq i} \sum_{x_j} P \bigl( \bigwedge_{j = 1}^{n} X_j =
    x_j \bigr) \ \forall\ i \in 1 .. n
  \end{equation}
\end{thm}

I.e., the marginal probability distribution of $X_i$ is the sum of the joint
probability distributions of $X$ over all values $x_j$ where $j \neq i$.

\subsubsection{Independence}

By definition~\ref{def:3:Independence}, if $X$ are \textit{independent}, then
the joint probability distribution of $X$ is defined by its \textit{marginal}
distributions.
Then, the number of values grows only linearly with the dimension of the random
variables.

\begin{dfn}[Independence]
  \label{def:3:Independence}
  Let $X = \{ X_i \mid i \in 1 .. j \}$ be a set of random variables.
  $X$ are independent if:
  \begin{equation}
    P \bigl( \bigwedge_{i = 1}^{n} X_i = x_i \bigr) = \prod_{i = 1}^{n} P(X_i =
    x_i) \ \forall \ x_i \in \{ X_i(w) : w \in W \}
  \end{equation}
\end{dfn}

\subsubsection{Conditional distributions}

We cannot generally assume that random variables are
independent.
But some random variables are not directly dependent on others.
The formalization of this intuition helps to make probabilistic reasoning
computationally feasible.

\begin{dfn}[Conditional distribution]
  Let $X = \{ X_i \mid i \in 1 .. j \}$ be a set of random variables.
  Without loss of generality, let $\{ X_1, X_2 \} \in \mathbb{X}$ form a
  partition of $X$:
  \begin{align*}
    X_1 & = \{ X_i \mid i \in 1 .. k - 1 \}
    \\
    X_2 & = \{ X_i \mid i \in k .. j \}
  \end{align*}
  The conditional probability of $\bigwedge_{i = 1}^{k - 1} X_i = x_i$
  given $\bigwedge_{i = k}^{j} X_i = x_i$ is:
  \begin{equation}
    \label{eqn:3:ConditionalDistribution}
    P \bigl( \bigwedge_{i = 1}^{k - 1} X_i = x_i \mid \bigwedge_{i = k}^{j} X_i = x_i \bigr)
    = \frac{P(\bigwedge_{i = 1}^{j} X_i = x_i)}{P(\bigwedge_{i = k}^{j} X_i = x_i)}
  \end{equation}
  The denominator of equation~\ref{eqn:3:ConditionalDistribution} is the joint
  probability distribution:
  \begin{equation}
    P(\bigwedge_{i = k}^{j} X_i = x_i) = \sum_{x_1} \ldots \sum_{x_{j - 1}}
    P(\bigwedge_{l = 1}^{j} X_l = x_l)
  \end{equation}
\end{dfn}

I.e., if an agent receives information that $\bigwedge_{l = k}^{j} X_l = x_l$,
then to update its probabilities for the other random variables $\{ X_i \mid i
  \in 1 .. k - 1 \}$, it evaluates the conditional probability of the other
random variables given the information.

\begin{dfn}[Conditional independence]
  Let $X = \{ X_i \mid i \in 1 .. j \}$ be a set of random variables.
  Without loss of generality, let $\{ X_1, X_2, X_3 \} \in \mathbb{X}$ form a
  partition of $X$:
  \begin{align*}
    X_1 & = \{ X_i \mid i \in 1 .. k - 1 \}
    \\
    X_2 & = \{ X_i \mid i \in k .. l - 1 \}
    \\
    X_3 & = \{ X_i \mid i \in l .. j \}
  \end{align*}
  Then $X_1$ are conditionally independent of $X_3$ given $X_2$ if:
  \begin{multline}
    P \bigl( \bigwedge_{i = 1}^{k - 1} X_i = x_i \mid \bigwedge_{i = k}^{j} X_i =
    x_i \bigr)
    \\
    = P \bigl( \bigwedge_{i = 1}^{k - 1} X_i = x_i \mid \bigwedge_{i
      = k}^{l - 1} X_i = x_i \bigr) \ \forall\ x_i \in \{ X_i(w) : w \in W \}
  \end{multline}
\end{dfn}

I.e., if an agent knows the values
of $\{ X_i \mid i \in k .. l - 1 \}$, then it can ignore the values of $\{ X_i
  \mid i \in l .. j \}$ when updating its probabilities for $\{ X_i \mid i \in 1
  .. k - 1 \}$.

\subsubsection{Bayesian networks}

A Bayesian network is a graphical model of probabilistic reasoning with
multiple random variables.
It is a compromise between independence and dependence: it assumes
\textit{independence where possible} and \textit{dependency where necessary}.

\begin{dfn}[Directed graph]
  A directed graph is an ordered pair $(V, E)$ where
  $V = \{ v_i \mid i \in 1 .. j \}$ is a set of vertices and $E$ is a binary
  relation on $V$ that defines a set of edges.
\end{dfn}

\begin{dfn}[Directed acyclic graph]
  A directed graph $(V, E)$ is acyclic if there is no sequence of vertices
  $v_i \ldots v_k$ where $v_i = v_k$ and $(v_l, v_{l + 1}) \in E$ for all
  $l \in 1 .. k - 1$.
\end{dfn}

\begin{dfn}[Bayesian network]
  \label{def:3:BayesianNetwork}
  A Bayesian network is:
  \begin{itemize}
    \item a directed acyclic graph $(V, E)$ where each vertex $v_i \in V$ is a
          random variable $X_i : W \to \Omega$ and $(X_i, X_j) \in E$
          only if $j < i$; and
    \item a joint probability distribution on $\{ X_i \mid i \in 1 .. k \}$
          where:
          \begin{equation}
            P(X_i \mid \{ X_j \mid j \in 1 .. i - 1 \}) = P(X_i \mid \Pi(X_i))
          \end{equation}
          and $\Pi(X_i) = \{ X_k : (X_k, X_i) \in E \}$ is the set of
          parent vertices of $X_i$.
  \end{itemize}
\end{dfn}

For a Bayesian network, we assume that $X_i$ is conditionally independent of
its \textit{indirect} causes $\{ X_j \mid j \in 1 .. i - 1 \} - \Pi(X_i)$ given
its \textit{direct} causes $\Pi(X_i)$.
The joint probability distribution of $X$ is determined by its conditional
distributions $P(X_i \mid \Pi(X_i)) \ \forall\ i \in 1 .. k$
(theorem~\ref{thm:3:BayesianNetworkJPD}).

\begin{thm}[Joint probability distribution of a Bayesian network]
  \label{thm:3:BayesianNetworkJPD}
  \begin{equation}
    P(X) = \prod_{i = 1}^{k} P(X_i \mid \Pi(X_i))
  \end{equation}
  \begin{proof}
    Trivially:
    \begin{equation}
      P(\{ X_i \mid i \in 1 .. k \})
      = \prod_{i = 1}^{k} P(X_i \mid \{ X_j \mid j \in 1 .. i - 1 \})
    \end{equation}
    By the conditional independence assumptions of
    definition~\ref{def:3:BayesianNetwork}:
    \begin{equation}
      \prod_{i = 1}^{k} P(X_i \mid \{ X_j \mid j \in 1 .. i - 1 \})
      = \prod_{i = 1}^{k} P(X_i \mid \Pi(X_i))
    \end{equation}
  \end{proof}
\end{thm}

\subsubsection{Numbers of probability values}

For $n$ binary random variables, the number of probability values in the joint
probability distribution of:
\begin{itemize}
  \item a fully dependent model is $2^n - 1$;
  \item a fully independent model is $2n - 1$; and
  \item a Bayesian network is $\sum_{i = 1}^{n} 2^{|\Pi(X_i)|}$.
\end{itemize}

\begin{thm}[Number of probability values for a Bayesian network]
  For $n$ binary random variables, the joint probability distribution of a
  Bayesian network has between $n$ and $2^n - 1$ values.
  \begin{proof}
    The minimum of $|\Pi(X_i)|$ is 0, in which case:
    \begin{equation}
      \sum_{i = 1}^{n} 2^{|\Pi(X_i)|} = \sum_{i = 1}^{n} 2^0 = n
    \end{equation}
    The maximum of $|\Pi(X_i)|$ is $i - 1$, in which case:
    \begin{equation}
      \sum_{i = 1}^{n} 2^{|\Pi(X_i)|}
      = \sum_{i = 1}^{n} 2^{i - 1}
      = \sum_{i = 0}^{n - 1} 2^i
      = \frac{1 (1 - 2^n)}{1 - 2}
      = 2^n - 1
    \end{equation}
  \end{proof}
\end{thm}

For $n$ random variables, where the random variable $X_i$ has $k_i$ possible
values, i.e., $|\{ X_i(w) : w \in W \}| = k_i \ \forall\ i \in 1 .. n$, the
number of values in:
\begin{itemize}
  \item the conditional distributions $P(X_i \mid \Pi(X_i))$ is
        $(k_i - 1) \prod_{X_j \in \Pi(X_i)} k_j$; and
  \item the joint probability distribution of a Bayesian network is

        $\sum_{i = 1}^{n} (k_i - 1) \prod_{X_j \in \Pi(X_i)} k_j$.
\end{itemize}

Generally, the number of values for a Bayesian network is inversely
proportional to the numbers of direct causes of the random variables.
