\section{Probability theory}
\subsection{Introduction}

Probability theory is the best-known theory of uncertainty.
Definition~\ref{def:2:ProbabilityMeasure} states that probability measures are additive uncertainty measures.

\begin{dfn}[Probability measure]
  \label{def:2:ProbabilityMeasure}

  A probability measure is a function $P : 2^W \to [0, 1]$ such that:
  \begin{enumerate}
    \item[P1] $P(W) = 1$ and $P(\emptyset) = 0$
    \item[P2] $A \cap B = \emptyset \Rightarrow P(A \cup B) = P(A) + P(B)$
  \end{enumerate}
\end{dfn}


Additive uncertainty measures are monotonic.

\begin{thm}[Monotonicity]
  \begin{equation}
    \label{eqn:2:Monotonicity}
    A \subseteq B \Rightarrow P(A) \leq P(B)
  \end{equation}
  \begin{proof}
    Let $A, B \subseteq W$ such that $A \subseteq B$.
    By definition, $B = A \cup(B \cap A^c)$ and $A \cap(B \cap A^c) = \emptyset$.
    From definition~\ref{def:2:ProbabilityMeasure},
    $P(B) = P(A) + P(B \cap A^c)$ and $P(B \cap A^c) \in [0, 1]$, hence $P(B) \geq P(A)$.
  \end{proof}
\end{thm}

\begin{thm}[Complement]
  \label{thm:2:Complement}
  \begin{equation}
    \label{eqn:2:Complement}
    P(A^c) = 1 - P(A)
  \end{equation}
  \begin{proof}
    By definition, $A \cup A^c = W$ and $A\cap A^c = \emptyset$.
    From definition~\ref{def:2:ProbabilityMeasure},
    $P(A \cup A^c) = P(A) + P(A^c) = 1$, hence \ref{eqn:2:Complement}.
  \end{proof}
\end{thm}

\begin{thm}[General additivity]
  \label{thm:2:GeneralAdditivity}
  \begin{equation}
    \label{eqn:2:GeneralAdditivity}
    P(A \cup B) = P(A) + P(B) - P(A \cap B)
  \end{equation}
  \begin{proof}
    By definition, $A = (A \cap B) \cup (A \cap B^c)$, $B = (B \cap A) \cup (B \cap A^c)$
    and $A \cup B = (A \cap B) \cup (A \cap B^c) \cup (B \cap A^c)$.
    $A \cap B$, $A \cap B^c$, and $B \cap A^c$ do not overlap.
    Hence, from definition~\ref{def:2:ProbabilityMeasure}:
    \begin{align}
      P(A)        & = P(A \cap B) + P(A \cap B^c) \label{eqn:2:GeneralAdditivity1}                 \\
      P(B)        & = P(B \cap A) + P(B \cap A^c) \label{eqn:2:GeneralAdditivity2}                 \\
      P(A \cup B) & = P(A \cap B) + P(A \cap B^c) + P(B \cap A^c) \label{eqn:2:GeneralAdditivity3}
    \end{align}
    Substituting \ref{eqn:2:GeneralAdditivity1} and \ref{eqn:2:GeneralAdditivity2}
    into \ref{eqn:2:GeneralAdditivity3} gives \ref{eqn:2:GeneralAdditivity}.
  \end{proof}
\end{thm}

\begin{thm}[Theorem of total probability]
  \label{thm:2:TotalProbability}
  \begin{equation}\label{eqn:2:TotalProbability}
    P(A) = P(A \mid B) P(B) + P(A \mid B^c) P(B^c)
  \end{equation}
\end{thm}

\begin{dfn}[Probability distribution]
  \label{def:2:ProbabilityDistribution}
  A probability distribution is a function $P(\{w\})$ or $P(w) : w \in W$.
\end{dfn}

A probability measure is completely determined by its associated probability distribution.
Let $A \subseteq W$ be a proposition.
By definition~\ref{def:2:ProbabilityMeasure}:

\begin{equation}
  \label{eqn:2:ProbabilityDistribution}
  P(A) = P( \bigcup_{w \in A} \{w\}) = \sum_{w \in A} P(w)
\end{equation}

It is more practical to work with probability measures than general uncertainty measures.
For $n = |W| - 1$, a general uncertainty measure is defined by $2^n - 2$ values, whereas a probability measure is defined by $n - 1$ values.

\subsection{Updating probabilities and Bayesian reasoning}

If an agent learns that a proposition $A$ is true, then it should update its probability distribution to reflect that $w^* \in A$.
If an agent learns that a proposition $A$ is false, then it cannot update its probability distribution.

\begin{dfn}[Conditional probability]
  \label{def:2:ConditionalProbability}

  Let $P$ be a probability measure such that $P(B)>0$.
  The conditional probability of $A$ given $B$ is:
  \begin{equation}\label{eqn:2:ConditionalProbability}
    P(A \mid B) = \frac{P(A \cap B)}{P(B)}
  \end{equation}
\end{dfn}

Let $H = \{H_i \mid i \in 1 .. k\}\subseteq W$ be a set of hypotheses.
$H$ are:
\begin{itemize}
  \item \textit{mutually exclusive} if $H_i \cap H_j = \emptyset\ \forall\ i \neq j$; and
  \item \textit{exhaustive} if $\bigcup_{i = 1}^{k} H_i = W$.
\end{itemize}
In the context of Bayes' theorem:
\begin{itemize}
  \item $P(H_i \mid E)$ are the \textit{posterior} probabilities;
  \item $P(H_i)$ are the \textit{prior} probabilities; and
  \item $P(E \mid H_i)$ are the \textit{likelihoods}.
\end{itemize}

It is generally impractical to evaluate the posterior probabilities.
But it is more practical to estimate the prior probabilities and the likelihoods.
Bayes' theorem can be used to estimate the posterior probabilities.

\begin{thm}[Bayes' theorem]
  \label{thm:2:BayesTheorem}

  Let $\{H_i \mid i \in 1 .. k\}\subseteq W$ such that
  $H_i \cap H_j = \emptyset\ \forall\ i \neq j$ and $\bigcup_{i = 1}^{k} H_i = W$.
  Then, for $E \subseteq W$ such that $P(E) > 0$:
  \begin{equation}
    \label{eqn:2:BayesTheorem}
    P(H_i \mid E)
    = \frac{P(E \mid H_i)P(H_i)}{\sum_{j = 1}^{k} P(E \mid H_j) P(H_j)}\
    \forall\ i \in 1 .. k
  \end{equation}
  \begin{proof}
    By definition \ref{def:2:ConditionalProbability}:
    \begin{equation}
      \label{eqn:2:BayesTheorem1}
      P(E \mid H_i) = \frac{P(H_i \cap E)}{P(H_i)}\ ,\
      P(H_i \mid E) = \frac{P(E \cap H_i)}{P(E)}
    \end{equation}
    Therefore:
    \begin{equation}
      \label{eqn:2:BayesTheorem2}
      P(H_i \mid E) = \frac{P(E \mid H_i) P(H_i)}{P(E)}
    \end{equation}
    $\{H_i\mid i\in 1 .. k\}$ is a partition of $W$, hence:
    \begin{equation}
      \label{eqn:2:BayesTheorem3}
      \sum_{i = 1}^{k}P(H_i \mid E) = 1
    \end{equation}
    Substituting \ref{eqn:2:BayesTheorem3} into \ref{eqn:2:BayesTheorem2} gives:
    \begin{equation}
      \label{eqn:2:BayesTheorem5}
      1 = \sum_{i = 1}^{k}\frac{P(E \mid H_i)P(H_i)}{P(E)}
      = \frac{\sum_{i = 1}^{k} P(E \mid H_i) P(H_i)}{P(E)}
    \end{equation}
    Substituting \ref{eqn:2:BayesTheorem5} into \ref{eqn:2:BayesTheorem3} gives \ref{eqn:2:BayesTheorem}.
  \end{proof}
\end{thm}

Conditional inference applies a general rule to a specific instance:
\begin{itemize}
  \item $P(A \mid B)$ is a conditional probability defined on a set of instances; and
  \item $P(B)$ is a probability defined on a specific instance.
\end{itemize}

In Bayes' theorem, proposition $E$ is certainly true, i.e., $w^* \in E$.
Jeffrey's rule is a generalisation of Bayes' theorem where $P(E) \in [0, 1]$.
Its result is a new probability measure $P^\prime$.

\begin{thm}[Jeffrey's rule]
  \label{thm:2:JeffreysRule}
  \begin{equation}\label{eqn:2:JeffreysRule}
    P^\prime(H) = P(H \mid E) P^\prime(E) + P(H \mid E^\prime) (1 - P^\prime(E))
  \end{equation}
\end{thm}

Theorem~\ref{thm:2:JeffreysRule} is an extension of \ref{thm:2:TotalProbability} that permits inference about specific instances from general conditional probabilities.

Bayesian reasoning strongly depends on the prior probabilities.
Any posterior probability can be obtained by selecting an appropriate prior probability.
I.e., the likelihoods alone do not determine the posterior probabilities.

\begin{dfn}[Bayes factor]
  \label{def:2:BayesFactor}

  Given evidence $E$ and a hypothesis $H$, the Bayes factor is:
  \begin{equation}\label{eqn:2:BayesFactor}
    F = \frac{P(E \mid H)}{P(E \mid H^c)}
  \end{equation}
\end{dfn}

\subsection{Prior probabilities}

Bayesian reasoning is difficult without prior knowledge of the hypotheses.
The most common approach to this problem is \textit{Laplace's principle of insufficient reasoning}:
\begin{displayquote}
  In the absence of any other information, all hypotheses under consideration should be assumed to be equally probable, i.e., the probability distribution should be \textit{uniform}.
\end{displayquote}
The uniform distribution is the least informative (theorem~\ref{thm:3:MaximumEntropyDistribution}).

Probability theory conflates uncertainty (a lack of knowledge as to the true possible world, quantified by a probability measure) and ignorance (a lack of knowledge that makes it difficult to quantify one's beliefs).
A different approach is to use a theory of uncertainty that differentiates between uncertainty and ignorance, e.g., Dempster-Shafer theory.
